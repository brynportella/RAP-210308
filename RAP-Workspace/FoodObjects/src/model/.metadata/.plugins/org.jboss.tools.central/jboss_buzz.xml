<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Deploy Quarkus everywhere with Red Hat Enterprise Linux (RHEL)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/RHn0yrSElvY/" /><category term="Java" /><category term="Kubernetes" /><category term="Linux" /><category term="Quarkus" /><category term="cloud native java" /><category term="edge development" /><category term="Quarkus on RHEL" /><author><name>Syed M Shaaf</name></author><id>https://developers.redhat.com/blog/?p=888547</id><updated>2021-04-07T13:05:19Z</updated><published>2021-04-07T13:05:19Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="/topics/enterprise-java/"&gt;Java&lt;/a&gt; is one of the most popular programming languages in the world. It has been among the &lt;a target="_blank" rel="nofollow" href="https://www.tiobe.com/tiobe-index/?hid=B4E841AA3BF5CD6D546F03D321E49994&amp;#38;wordfence_lh=1"&gt;top three languages&lt;/a&gt; used over the past two decades. Java powers millions of applications across many verticals and platforms. &lt;a target="_blank" rel="nofollow" href="/topics/linux/"&gt;Linux&lt;/a&gt; is widely deployed in data centers, edge networks, and the cloud.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/whats-new-quarkus-and-other-updates-red-hat-runtimes"&gt;Today we announced&lt;/a&gt; that &lt;a target="_blank" rel="nofollow" href="/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; is now available for all &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; customers. If you are running RHEL, you can easily use the &lt;a target="_blank" rel="nofollow" href="/products/quarkus/getting-started"&gt;Red Hat build of Quarkus&lt;/a&gt; in your Java applications. If you are developing applications on a &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; platform like &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, you can also use the Red Hat build of Quarkus &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/introducing-quarkus-red-hat-openshift"&gt;as of November 2020&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What is Quarkus, and how can you develop and deploy it on Red Hat Enterprise Linux? Read on to learn more. This article covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Using the Red Hat build of Quarkus on RHEL.&lt;/li&gt; &lt;li&gt;Running Quarkus in development mode.&lt;/li&gt; &lt;li&gt;Creating Java native executables with and without Podman.&lt;/li&gt; &lt;li&gt;Building application images with Podman on RHEL.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What is the Red Hat build of Quarkus?&lt;/h2&gt; &lt;p&gt;If you are not familiar with Quarkus, its tagline is “supersonic, subatomic Java.” And yes, Java is super fast. With Quarkus, Java is even more lightweight and straightforward for developer use.&lt;/p&gt; &lt;p&gt;Quarkus is a Kubernetes-native &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/cloud-native-apps/what-is-a-Java-framework"&gt;Java framework&lt;/a&gt; built for the Java Virtual Machine (JVM) and native compilation with GraalVM and Mandrel. Quarkus optimizes your Java code specifically for containers, making it an effective platform for &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/cloud"&gt;cloud&lt;/a&gt; environments like Red Hat OpenShift. Quarkus is designed to work with popular Java standards, frameworks, and libraries, including Eclipse MicroProfile, Spring, Apache Kafka, RESTEasy (JAX-RS), Hibernate ORM (JPA), Infinispan, Apache Camel.&lt;/p&gt; &lt;h2&gt;How to get started with Quarkus on RHEL&lt;/h2&gt; &lt;p&gt;There are multiple ways to start using Quarkus on RHEL. The &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/"&gt;Quarkus documentation&lt;/a&gt; provides a list of different approaches. All you need to do is get the artifacts from Red Hat’s Maven repo.&lt;/p&gt; &lt;p&gt;For newbies, it’s simple to get started with the &lt;a target="_blank" rel="nofollow" href="https://code.quarkus.redhat.com"&gt;project generator&lt;/a&gt; using a web browser or Maven plug-in, as shown in Figure 1. Once configured, you can download the ZIP file or copy the Maven command to run Quarkus on your machine.&lt;/p&gt; &lt;div id="attachment_889647" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a3eed7454.png"&gt;&lt;img aria-describedby="caption-attachment-889647" class="wp-image-889647 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a3eed7454-1024x705.png" alt="The Quarkus project generator at https://code.quarkus.redhat.com." width="640" height="441" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a3eed7454-1024x705.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a3eed7454-300x207.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a3eed7454-768x529.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a3eed7454.png 1241w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889647" class="wp-caption-text"&gt;Figure 1: The Quarkus project generator at code.quarkus.redhat.com.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 1 shows all of the extensions that are supported in &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/support/offerings/techpreview/"&gt;technical preview&lt;/a&gt; and available for use. Quarkus has a vast ecosystem of extensions that help developers write applications, such as Kafka, Hibernate Reactive, Panache, and Spring.&lt;/p&gt; &lt;h2&gt;An example edge application&lt;/h2&gt; &lt;p&gt;It’s common for examples to use a CRUD application. However, we will look at an edge application that takes data from the device. This example illustrates a typical use case for Quarkus and RHEL at the network edge.&lt;/p&gt; &lt;p&gt;I have created a basic application that can run on a lightweight, resource-efficient RHEL server on the edge. Following is a breakdown of the data flow from devices to the front end. Figure 2 also shows the high-level architecture.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The devices send data to an MQTT broker.&lt;/li&gt; &lt;li&gt;Quarkus uses reactive messaging and channels to receive, process, and showcase those messages on a browser-based front end. Data comes in real time via a channel.&lt;/li&gt; &lt;li&gt;The front end uses REST and JavaScript.&lt;/li&gt; &lt;/ul&gt; &lt;div id="attachment_889667" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a47e734a7.png"&gt;&lt;img aria-describedby="caption-attachment-889667" class="wp-image-889667" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a47e734a7.png" alt="A high-level architecture diagram of Quarkus." width="640" height="641" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a47e734a7.png 622w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a47e734a7-150x150.png 150w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a47e734a7-300x300.png 300w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889667" class="wp-caption-text"&gt;Figure 2: A high-level architecture diagram of Quarkus.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To follow along or try it out, see the source code for this application in the &lt;a target="_blank" rel="nofollow" href="https://github.com/sshaaf/quarkus-edge-mqtt-demo"&gt;example repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s deploy this application on our RHEL server.&lt;/p&gt; &lt;h3&gt;Starting the MQTT broker with Podman&lt;/h3&gt; &lt;p&gt;RHEL has a daemonless container engine. What does this mean? RHEL uses Podman as the container engine. The Podman architecture allows you to run the containers under the user that is starting the container (the fork/exec model), and that user does not need root privileges. Because Podman has a daemonless architecture, a user running Podman can only see and modify their own containers. There is no common daemon that communicates with the command-line interface (CLI) tool.&lt;/p&gt; &lt;p&gt;We will use Podman throughout this example. You can learn more about Podman in this &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/building_running_and_managing_containers/index"&gt;guide to Linux containers on Red Hat Enterprise Linux 8&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For this demo, we will use the Mosquitto message broker. Mosquitto is lightweight and suitable for all devices, from low-power, single-board computers to full servers. Let’s start an instance of Mosquitto using Podman:&lt;/p&gt; &lt;pre&gt;podman run --name mosquitto \ --rm -p "9001:9001" -p "1883:1883" \ eclipse-mosquitto:1.6.2&lt;/pre&gt; &lt;h3&gt;Building our application with Quarkus&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This example assumes you have the latest Red Hat build of OpenJDK, &lt;a href="https://developers.redhat.com/products/openjdk/download"&gt;OpenJDK 11&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Next, we will spin up our application. You can use any integrated development environment (IDE) to develop with Quarkus. Most IDEs allow you to use the Quarkus Tools extension, which makes it easy for developers to create Quarkus applications.&lt;/p&gt; &lt;p&gt;To run Quarkus in development mode, follow these steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open a terminal from your RHEL machine or any IDE.&lt;/li&gt; &lt;li&gt;&lt;code&gt;cd&lt;/code&gt; into the project directory: &lt;code&gt;https://github.com/sshaaf/quarkus-edge-mqtt-demo&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Run this command: &lt;code&gt;mvn quarkus&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The output should look similar to what you see in Figure 3.&lt;/p&gt; &lt;div id="attachment_889687" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4d958d3c.png"&gt;&lt;img aria-describedby="caption-attachment-889687" class="wp-image-889687 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4d958d3c-1024x385.png" alt="Quarkus development mode output." width="640" height="241" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4d958d3c-1024x385.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4d958d3c-300x113.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4d958d3c-768x289.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4d958d3c.png 1079w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889687" class="wp-caption-text"&gt;Figure 3: Quarkus development mode output.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Open your browser and navigate to &lt;a target="_blank" rel="nofollow" href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt;. You should see the main page for our application reporting real-time data from our emulated device, as shown in Figure 4. In this case, our emulated device, ESP8266-01, throws temperature and heat measurements in JSON format from the device into the MQTT broker. The JSON is then picked up as a reactive channel and throws that data out after processing into the stream. The browser reads the stream and displays the data in real time. You can easily change the emulated device to a real one; however, the data thrown must be in the correct JSON format.&lt;/p&gt; &lt;div id="attachment_889697" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4fc880e9.png"&gt;&lt;img aria-describedby="caption-attachment-889697" class="wp-image-889697 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4fc880e9-1024x521.png" alt="Real-time data from the emulated device." width="640" height="326" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4fc880e9-1024x521.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4fc880e9-300x153.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a4fc880e9-768x391.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889697" class="wp-caption-text"&gt;Figure 4: Real-time data from the emulated device.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Using Quarkus in development mode&lt;/h3&gt; &lt;p&gt;Now you have a running application in development mode on your RHEL machine. What do developers gain from using Quarkus in development mode? Benefits include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero configuration, live code, and the ability to reload in the blink of an eye. If you change any of your Java files, you don’t need to reload the entire environment. Quarkus understands!&lt;/li&gt; &lt;li&gt;Based on standards, but not limited.&lt;/li&gt; &lt;li&gt;Unified configuration.&lt;/li&gt; &lt;li&gt;Streamlined code for the 80% common usages, flexible for the 20%.&lt;/li&gt; &lt;li&gt;Easy native executable generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Red Hat build of Quarkus &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/1.11/html/release_notes_for_red_hat_build_of_quarkus_1.11/index"&gt;1.11 release&lt;/a&gt; introduced an awesome developer console called the Quarkus Dev UI. You can access the Quarkus Dev UI in dev mode by navigating to &lt;code&gt;http://localhost:8080/q/dev&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In the Quarkus Dev UI, select &lt;strong&gt;SmallRye Reactive Messaging Channels&lt;/strong&gt; from the extensions shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_889707" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a53c7f6e1.png"&gt;&lt;img aria-describedby="caption-attachment-889707" class="wp-image-889707" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a53c7f6e1.png" alt="Extensions listed in the Quarkus Dev UI." width="640" height="396" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a53c7f6e1.png 849w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a53c7f6e1-300x186.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a53c7f6e1-768x475.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889707" class="wp-caption-text"&gt;Figure 5: Extensions in the Quarkus Dev UI.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;From there, you will see the reactive streams that our edge device is using (see Figure 6).&lt;/p&gt; &lt;div id="attachment_889717" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a582a8ff3.png"&gt;&lt;img aria-describedby="caption-attachment-889717" class="wp-image-889717 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a582a8ff3-1024x177.png" alt="Reactive streams extensions and list of streams." width="640" height="111" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a582a8ff3-1024x177.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a582a8ff3-300x52.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a582a8ff3-768x133.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a582a8ff3.png 1535w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889717" class="wp-caption-text"&gt;Figure 6: Reactive streams extensions and list of streams.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;For more details about development mode, see &lt;a target="_blank" rel="nofollow" href="/blog/2021/02/11/enhancing-the-development-loop-with-quarkus-remote-development/"&gt;Enhancing the development loop with Quarkus remote development&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Build the application image with Podman&lt;/h3&gt; &lt;p&gt;You can create a native binary for your platform by running the &lt;code&gt;-Pnative&lt;/code&gt; directive with Maven.&lt;/p&gt; &lt;p&gt;However, you might not have the entire compilation environment set up—for instance, you haven&amp;#8217;t installed Mandrel or GraalVM. In that case, you can use your container runtime to build the native image. The simplest way to do this is by running the following command:&lt;/p&gt; &lt;pre&gt;./mvnw package -Pnative -Dquarkus.native.container-build=true  -Dquarkus.native.container-runtime=podman&lt;/pre&gt; &lt;p&gt;Quarkus will pick up the default container runtime (in this case, Podman).&lt;/p&gt; &lt;p&gt;You can also specify &lt;code&gt;-Dquarkus.native.container-runtime=podman&lt;/code&gt; to explicitly select Podman. It takes a few minutes to build the image for optimizing the Quarkus application through dead code elimination, class scanning, reflections, and build proxies. Quarkus will optimize the application not just for native images, but also for JVM mode. As a result, you will see fast startup times and a low memory footprint. Figure 7 shows the process from compilation to executables. For more details, take a look at the &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/blog/tag/performance/"&gt;Quarkus performance blogs&lt;/a&gt;.&lt;/p&gt; &lt;div id="attachment_889727" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a5bfda0fc.png"&gt;&lt;img aria-describedby="caption-attachment-889727" class="wp-image-889727" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a5bfda0fc.png" alt="The Quarkus build process." width="640" height="255" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a5bfda0fc.png 986w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a5bfda0fc-300x120.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/img_6065a5bfda0fc-768x306.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889727" class="wp-caption-text"&gt;Figure 7: The Quarkus build process.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can also limit the amount of memory used during native compilation by setting the &lt;code&gt;quarkus.native.native-image-xmx&lt;/code&gt; configuration property. Note that setting low memory limits might increase the build time. You can also use Podman to create a container image with our binary.&lt;/p&gt; &lt;p&gt;Under &lt;code&gt;src/main&lt;/code&gt;, Quarkus pregenerates different Dockerfiles for your application. Here, we will use the native Dockerfile because we already created a native binary. Execute the following command in our project home directory:&lt;/p&gt; &lt;pre&gt;podman build -f src/main/docker/Dockerfile.native -t sshaaf/quarkus-edge-mqtt . &lt;/pre&gt; &lt;p&gt;Finally, run the following command to launch the container on your RHEL machine:&lt;/p&gt; &lt;pre&gt;podman run -i --rm -p 8080:8080 sshaaf/quarkus-edge-mqtt &lt;/pre&gt; &lt;p&gt;Return to &lt;a target="_blank" rel="nofollow" href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt;. You should now see the application running and displaying incoming data from our device.&lt;/p&gt; &lt;h2&gt;Quarkus resources&lt;/h2&gt; &lt;p&gt;Quarkus is a Java framework suitable for multiple use cases, whether you are running applications on edge gateways, creating serverless functions, or deploying on cloud environments like Kubernetes and Red Hat OpenShift. Quarkus is easy and enjoyable for developers to use, and it improves Java application performance for the cloud.&lt;/p&gt; &lt;p&gt;If you want to learn more about Quarkus, here are some helpful resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/1.11/html/release_notes_for_red_hat_build_of_quarkus_1.11/index"&gt;Release notes for Red Hat build of Quarkus 1.11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/1.11/html/getting_started_with_quarkus/index"&gt;Getting started with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/"&gt;Product documentation for Red Hat build of Quarkus 1.11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="/courses/quarkus"&gt;Developing with Quarkus course&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="/books/practising-quarkus"&gt;Practising Quarkus e-book&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="/books/understanding-quarkus"&gt;Understanding Quarkus e-book&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="/cheat-sheets/quarkus-kubernetes-i"&gt;Quarkus Cheat Sheet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://dzone.com/refcardz/quarkus-1?chapter=1"&gt;Quarkus DZone RefCard&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="blog/2019/03/07/quarkus-next-generation-kubernetes-native-java-framework/"&gt;Introducing Quarkus: A next-generation Kubernetes native Java framework&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/07/deploy-quarkus-everywhere-with-red-hat-enterprise-linux-rhel/"&gt;Deploy Quarkus everywhere with Red Hat Enterprise Linux (RHEL)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/RHn0yrSElvY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Java is one of the most popular programming languages in the world. It has been among the top three languages used over the past two decades. Java powers millions of applications across many verticals and platforms. Linux is widely deployed in data centers, edge networks, and the cloud. Today we announced that Quarkus is now [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/07/deploy-quarkus-everywhere-with-red-hat-enterprise-linux-rhel/"&gt;Deploy Quarkus everywhere with Red Hat Enterprise Linux (RHEL)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/07/deploy-quarkus-everywhere-with-red-hat-enterprise-linux-rhel/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">888547</post-id><dc:creator>Syed M Shaaf</dc:creator><dc:date>2021-04-07T13:05:19Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/07/deploy-quarkus-everywhere-with-red-hat-enterprise-linux-rhel/</feedburner:origLink></entry><entry><title>Securely connect Red Hat Integration Service Registry with Red Hat AMQ Streams</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/PEfroeCHkfw/" /><category term="Event-Driven" /><category term="Java" /><category term="Kubernetes" /><category term="Security" /><category term="amq streams" /><category term="Apicurio" /><category term="Authentication" /><category term="Authorization" /><category term="Kafka cluster" /><author><name>Roman Martin Gil</name></author><id>https://developers.redhat.com/blog/?p=779427</id><updated>2021-04-07T07:00:20Z</updated><published>2021-04-07T07:00:20Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="/blog/2019/12/16/getting-started-with-red-hat-integration-service-registry/"&gt;Red Hat Integration Service Registry&lt;/a&gt; is a datastore based on the &lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/"&gt;Apicurio&lt;/a&gt; open source project. In my previous article, I showed you &lt;a target="_blank" rel="nofollow" href="/blog/2021/02/15/integrating-spring-boot-with-red-hat-integration-service-registry/"&gt;how to integrate Spring Boot with Service Registry&lt;/a&gt;. In this article, you&amp;#8217;ll learn how to connect Service Registry to a secure &lt;a target="_blank" rel="nofollow" href="/products/amq/overview"&gt;Red Hat AMQ Streams&lt;/a&gt; cluster.&lt;/p&gt; &lt;h2&gt;Connecting Service Registry with AMQ Streams&lt;/h2&gt; &lt;p&gt;Service Registry includes a set of pluggable storage options for storing APIs, rules, and validations. The &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;-based storage option, provided by &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;Red Hat AMQ Streams&lt;/a&gt;, is suitable for production environments where persistent storage is configured for a Kafka cluster running on &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Security is not optional in a production environment, and AMQ Streams must provide it for each component you connect to. Security is defined by &lt;em&gt;authentication&lt;/em&gt;, which ensures a secure client connection to the Kafka cluster, and &lt;em&gt;authorization&lt;/em&gt;, specifying which users can access which resources. I will show you how to set up authentication and authorization with AMQ Streams and Service Registry.&lt;/p&gt; &lt;h2&gt;The AMQ Streams Operators&lt;/h2&gt; &lt;p&gt;AMQ Streams and Service Registry provide a set of &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/operators"&gt;OpenShift Operators&lt;/a&gt; available from the &lt;a target="_blank" rel="nofollow" href="https://operatorhub.io/"&gt;OpenShift OperatorHub&lt;/a&gt;. Developers use these Operators to package, deploy, and manage OpenShift application components.&lt;/p&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/amq_streams_on_openshift_overview/index#overview-components_str"&gt;AMQ Streams Operators&lt;/a&gt; provide a set of custom resource definitions (CRDs) to describe the components of a Kafka deployment. These objects—namely, Zookeeper, Brokers, Users, and Connect—provide the API that we use to manage our Kafka cluster. AMQ Streams Operators manage authentication, authorization, and the user&amp;#8217;s life cycle.&lt;/p&gt; &lt;p&gt;The AMQ Streams &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/amq_streams_on_openshift_overview/index#overview-components-cluster-operator-str"&gt;Cluster Operator&lt;/a&gt; manages the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/using_amq_streams_on_openshift/index#type-Kafka-reference"&gt;Kafka schema reference&lt;/a&gt; resource, which declares the Kafka topology and features to use.&lt;/p&gt; &lt;p&gt;The AMQ Streams &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/amq_streams_on_openshift_overview/index#overview-concepts-user-operator-str"&gt;User Operator&lt;/a&gt; manages the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/using_amq_streams_on_openshift/index#type-KafkaUser-reference"&gt;KafkaUser schema reference&lt;/a&gt; resource. This resource declares a user for an instance of AMQ Streams, including the user&amp;#8217;s authentication, authorization, and quota definitions.&lt;/p&gt; &lt;h2&gt;The Service Registry Operator&lt;/h2&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/registry/docs/apicurio-registry/1.3.3.Final/getting-started/assembly-installing-registry-openshift.html#installing-registry-operatorhub"&gt;Service Registry Operator&lt;/a&gt; provides a set of CRDs to describe service registry deployment components such as storage, security, and replicas. Together, these objects provide the API to manage a Service Registry instance.&lt;/p&gt; &lt;p&gt;The Service Registry Operator uses the &lt;a target="_blank" rel="nofollow" href="https://github.com/Apicurio/apicurio-registry-operator/blob/master/deploy/crds/apicur.io_apicurioregistries_crd.yaml"&gt;ApicurioRegistry schema reference&lt;/a&gt; to manage the service registry life cycle. The &lt;code&gt;ApicurioRegistry&lt;/code&gt; declares the service registry topology and main features. The Apicurio Operator manages the &lt;code&gt;ApicurioRegistry&lt;/code&gt; object.&lt;/p&gt; &lt;h2&gt;Authentication with AMQ Streams&lt;/h2&gt; &lt;p&gt;Red Hat AMQ Streams supports the following authentication mechanisms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SASL SCRAM-SHA-512&lt;/li&gt; &lt;li&gt;Transport Layer Security (TLS) client authentication&lt;/li&gt; &lt;li&gt;OAuth 2.0 token-based authentication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These mechanisms are declared in the &lt;code&gt;authentication&lt;/code&gt; block in each listener&amp;#8217;s &lt;code&gt;Kafka&lt;/code&gt; definition. Each listener implements the authentication mechanism defined, so the client applications must authenticate with the mechanism identified.&lt;/p&gt; &lt;h3&gt;Two ways to authenticate an AMQ Streams cluster&lt;/h3&gt; &lt;p&gt;First off, let&amp;#8217;s see how to activate each mechanism in the AMQ Streams cluster. We need to identify in Service Registry the authentication mechanism activated in the AMQ Streams cluster. Service Registry only allows &lt;a target="_blank" rel="nofollow" href="https://tools.ietf.org/id/draft-melnikov-scram-sha-512-01.html"&gt;SCRAM-SHA-512&lt;/a&gt; and TLS as authentication mechanisms. I&amp;#8217;ll show you how to configure authentication using each of these mechanisms.&lt;/p&gt; &lt;h4&gt;Authenticating a Kafka cluster with SCRAM-SHA-512&lt;/h4&gt; &lt;p&gt;The following &lt;code&gt;Kafka&lt;/code&gt; definition declares a Kafka cluster secured with SCRAM-SHA-512 authentication for the secured listener (secured using the TLS protocol):&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: my-kafka spec: kafka: listeners: tls: authentication: type: scram-sha-512 &lt;/pre&gt; &lt;p&gt;Applying this configuration creates a set of secrets where the TLS certificates are stored. The secret we need to know to allow the secured connections is declared as &lt;code&gt;my-kafka-cluster-ca-cert&lt;/code&gt;. Note that we will need this value later on.&lt;/p&gt; &lt;p&gt;The following &lt;code&gt;KafkaUser&lt;/code&gt; definition declares a Kafka user with SCRAM-SHA-512 authentication:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaUser metadata: name: service-registry-scram labels: strimzi.io/cluster: my-kafka spec: authentication: type: scram-sha-512 &lt;/pre&gt; &lt;p&gt;Applying this configuration creates a secret (the user&amp;#8217;s name) and stores it where the user credentials are stored. This secret contains the generated password to authenticate to the Kafka cluster:&lt;/p&gt; &lt;pre&gt;$ oc get secrets NAME TYPE DATA AGE service-registry-scram Opaque 1 4s &lt;/pre&gt; &lt;h4&gt;Authenticating a Kafka cluster with TLS&lt;/h4&gt; &lt;p&gt;The following &lt;code&gt;Kafka&lt;/code&gt; definition declares a Kafka cluster secured with TLS authentication for the secured listener (secured with the TLS protocol):&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: my-kafka spec: kafka: listeners: tls: authentication: type: tls &lt;/pre&gt; &lt;p&gt;Applying this configuration creates a set of secrets where the TLS certificates are stored. The secret we need to know to allow the secured connections is declared as &lt;code&gt;my-kafka-cluster-ca-cert&lt;/code&gt;. Note that we will need this value later on.&lt;/p&gt; &lt;p&gt;The following &lt;code&gt;KafkaUser&lt;/code&gt; definition declares a Kafka user with TLS authentication:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaUser metadata: name: service-registry-tls labels: strimzi.io/cluster: my-kafka spec: authentication: type: tls &lt;/pre&gt; &lt;p&gt;Applying this configuration creates a secret (the user&amp;#8217;s name) and stores it where the user credentials are stored. This secret contains the valid client certificates to authenticate to the Kafka cluster:&lt;/p&gt; &lt;pre&gt;$ oc get secrets NAME TYPE DATA AGE Service-registry-tls Opaque 1 4s &lt;/pre&gt; &lt;h3&gt;Service Registry authentication&lt;/h3&gt; &lt;p&gt;To identify the authentication mechanism activated in the AMQ Streams cluster, we need to deploy the Service Registry accordingly, using the &lt;code&gt;ApicurioRegistry&lt;/code&gt; definition:&lt;/p&gt; &lt;pre&gt;apiVersion: apicur.io/v1alpha1 kind: ApicurioRegistry metadata: name: service-registry spec: configuration: persistence: "streams" streams: &lt;strong&gt;bootstrapServers&lt;/strong&gt;: "my-kafka-kafka-bootstrap:&lt;strong&gt;9093&lt;/strong&gt;" &lt;/pre&gt; &lt;p style="padding-left: 40px"&gt;&lt;b&gt;Note&lt;/b&gt;: At the time of this writing, Service Registry can only connect to the AMQ Streams TLS listener (normally in port 9093) when the authentication mechanism is activated in that listener. The &lt;code&gt;ApicurioRegistry&lt;/code&gt; definition&amp;#8217;s &lt;code&gt;boostrapServers&lt;/code&gt; property must point to that listener port.&lt;/p&gt; &lt;h4&gt;Service Registry authentication using SCRAM-SHA-512&lt;/h4&gt; &lt;p&gt;The following &lt;code&gt;ApicurioRegistry&lt;/code&gt; definition declares a secured connection with a user with SCRAM-SHA-512 authentication:&lt;/p&gt; &lt;pre&gt;apiVersion: apicur.io/v1alpha1 kind: ApicurioRegistry metadata: name: service-registry spec: configuration: persistence: "streams" streams: bootstrapServers: "my-kafka-kafka-bootstrap:9093" security: scram: user: service-registry-scram passwordSecretName: service-registry-scram truststoreSecretName: my-kafka-cluster-ca-cert&lt;/pre&gt; &lt;p&gt;We need to identify the following values in this object:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;User&lt;/b&gt;: The username to be securely connected.&lt;/li&gt; &lt;li&gt;&lt;b&gt;PasswordSecretName&lt;/b&gt;:  The name of the secret where the password is saved.&lt;/li&gt; &lt;li&gt;&lt;b&gt;TruststoreSecretName&lt;/b&gt;: The name of secret with the certificate authority (CA) certificates for the deployed Kafka cluster.&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Service Registry authentication using TLS&lt;/h4&gt; &lt;p&gt;The following &lt;code&gt;ApicurioRegistry&lt;/code&gt; definition declares a secured connection with a user with a TLS authentication:&lt;/p&gt; &lt;pre&gt;apiVersion: apicur.io/v1alpha1 kind: ApicurioRegistry metadata: name: service-registry spec: configuration: persistence: "streams" streams: bootstrapServers: "my-kafka-kafka-bootstrap:9093" security: tls: keystoreSecretName: service-registry-tls truststoreSecretName: my-kafka-cluster-ca-cert&lt;/pre&gt; &lt;p&gt;The values that we need to identify in this object are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;KeystoreSecretName&lt;/b&gt;: The name of the user secret with the client certificates.&lt;/li&gt; &lt;li&gt;&lt;b&gt;TruststoreSecretName&lt;/b&gt;: The name of the secret with the CA certificates for the deployed Kafka cluster.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Authorization with AMQ Streams&lt;/h2&gt; &lt;p&gt;AMQ Streams supports authorization using &lt;code&gt;SimpleACLAuthorizer&lt;/code&gt; globally for all listeners used for client connections. This mechanism uses access control lists (ACLs) to define which users have access to which resources.&lt;/p&gt; &lt;p&gt;Denial is the default access control if authorization is applied in the Kafka cluster. The listener must declare different rules for each user that wishes to operate within the Kafka cluster.&lt;/p&gt; &lt;h3&gt;The Kafka definition&lt;/h3&gt; &lt;p&gt;The following &lt;code&gt;Kafka&lt;/code&gt; definition activates authorization in the Kafka cluster:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: my-kafka spec: kafka: authorization: type: simple&lt;/pre&gt; &lt;h3&gt;The KafkaUser definition&lt;/h3&gt; &lt;p&gt;An ACL is declared for each user in the &lt;code&gt;KafkaUser&lt;/code&gt; definition. The &lt;code&gt;acls&lt;/code&gt; section (see below) includes a list of resources, where each resource is declared as a new rule:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Resource type&lt;/strong&gt;: Identifies the type of object managed in Kafka; objects include topics, consumer groups, clusters, transaction IDs, and delegation tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resource name&lt;/strong&gt;: Identifies the resource where the rule is applied. The resource name could be defined as a &lt;i&gt;literal&lt;/i&gt;, to identify one resource, or as a &lt;i&gt;prefix pattern&lt;/i&gt;, to identify a list of resources.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operation&lt;/strong&gt;: Declares the kind of operations allowed. A full list of operations available for each resource type is available &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/using_amq_streams_on_openshift/index#simple-acl-str"&gt;here&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a Service Registry user to work successfully with our secured AMQ Streams cluster, we must declare the following rules specifying what the user is allowed to do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read its own consumer group.&lt;/li&gt; &lt;li&gt;Create, read, write, and describe on a global ID topic (&lt;code&gt;global-id-topic&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Create, read, write, and describe on a storage topic (&lt;code&gt;storage-topic&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Create, read, write, and describe on its own local changelog topics.&lt;/li&gt; &lt;li&gt;Describe and write transactional IDs on its own local group.&lt;/li&gt; &lt;li&gt;Read on a consumer offset topic (&lt;code&gt;__consumer_offsets&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Read on a transaction state topic (&lt;code&gt;__transaction_state&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Write idempotently on a cluster.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;The ACL definition&lt;/h3&gt; &lt;p&gt;Here is an example ACL definition:&lt;/p&gt; &lt;pre&gt; acls: # Group Id to consume information for the different topics used by the Service Registry. # Name equals to metadata.name property in ApicurioRegistry object - resource: type: group name: service-registry operation: Read # Rules for the Global global-id-topic - resource: type: topic name: global-id-topic operation: Read - resource: type: topic name: global-id-topic operation: Describe - resource: type: topic name: global-id-topic operation: Write - resource: type: topic name: global-id-topic operation: Create # Rules for the Global storage-topic - resource: type: topic name: storage-topic operation: Read - resource: type: topic name: storage-topic operation: Describe - resource: type: topic name: storage-topic operation: Write - resource: type: topic name: storage-topic operation: Create # Rules for the local topics created by our Service Registry instance # Prefix value equals to metadata.name property in ApicurioRegistry object - resource: type: topic name: service-registry- patternType: prefix operation: Read - resource: type: topic name: service-registry- patternType: prefix operation: Describe - resource: type: topic name: service-registry- patternType: prefix operation: Write - resource: type: topic name: service-registry- patternType: prefix operation: Create # Rules for the local transactionalsIds created by our Service Registry instance # Prefix equals to metadata.name property in ApicurioRegistry object - resource: type: transactionalId name: service-registry- patternType: prefix operation: Describe - resource: type: transactionalId name: service-registry- patternType: prefix operation: Write # Rules for internal Apache Kafka topics - resource: type: topic name: __consumer_offsets operation: Read - resource: type: topic name: __transaction_state operation: Read # Rules for Cluster objects - resource: type: cluster operation: IdempotentWrite&lt;/pre&gt; &lt;p&gt;Note that activating authorization in AMQ Streams does not affect the &lt;code&gt;ApicurioRegistry&lt;/code&gt; definition. It is only related to the correct ACL definitions in the &lt;code&gt;KafkaUser&lt;/code&gt; objects.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Connecting Service Registry&amp;#8217;s security capabilities to secure AMQ Streams clusters enables your production environment to prompt a warning about your security requirements. This article introduced the Service Registry and AMQ Streams components concerned with security requirements and showed you how to apply them successfully.&lt;/p&gt; &lt;p&gt;For a deeper understanding and analysis, please refer to the following references:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/using_amq_streams_on_openshift/index#security-str"&gt;Using AMQ Streams on OpenShift, Chapter 12: Security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/using_amq_streams_on_openshift/index#assembly-using-the-user-operator-str"&gt;Using the User Operator of AMQ Streams&lt;/a&gt; (Red Hat Integration 2020-Q2 documentation)&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q2/html/getting_started_with_service_registry/index"&gt;Getting started with Service Registry&lt;/a&gt; (Red Hat Integration 2020-Q2 documentation)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#38;linkname=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F07%2Fsecurely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams%2F&amp;#038;title=Securely%20connect%20Red%20Hat%20Integration%20Service%20Registry%20with%20Red%20Hat%20AMQ%20Streams" data-a2a-url="https://developers.redhat.com/blog/2021/04/07/securely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams/" data-a2a-title="Securely connect Red Hat Integration Service Registry with Red Hat AMQ Streams"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/07/securely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams/"&gt;Securely connect Red Hat Integration Service Registry with Red Hat AMQ Streams&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/PEfroeCHkfw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat Integration Service Registry is a datastore based on the Apicurio open source project. In my previous article, I showed you how to integrate Spring Boot with Service Registry. In this article, you&amp;#8217;ll learn how to connect Service Registry to a secure Red Hat AMQ Streams cluster. Connecting Service Registry with AMQ Streams Service [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/07/securely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams/"&gt;Securely connect Red Hat Integration Service Registry with Red Hat AMQ Streams&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/07/securely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">779427</post-id><dc:creator>Roman Martin Gil</dc:creator><dc:date>2021-04-07T07:00:20Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/07/securely-connect-red-hat-integration-service-registry-with-red-hat-amq-streams/</feedburner:origLink></entry><entry><title>C# 9 pattern matching</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/O0b7SuuZc4k/" /><category term=".NET" /><category term="C#" /><category term="Linux" /><category term=".NET 5" /><category term="C# 9" /><author><name>Tom Deseyn</name></author><id>https://developers.redhat.com/blog/?p=873797</id><updated>2021-04-06T07:00:57Z</updated><published>2021-04-06T07:00:57Z</published><content type="html">&lt;p&gt;The previous article in our C# 9 series looked at &lt;a target="_blank" rel="nofollow" href="/blog/2021/03/30/c-9-top-level-programs-and-target-typed-expressions/"&gt;top-level programs and target-typed expressions&lt;/a&gt;. In this article, we’ll cover new features for pattern matching. You can find an overview of the syntax offered by previous versions of &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C#&lt;/a&gt; in &lt;a target="_blank" rel="nofollow" href="/blog/2020/02/27/c-8-pattern-matching/"&gt;C# 8 pattern matching&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Type patterns&lt;/h2&gt; &lt;p&gt;When checking against a type, previous versions of C# required you to include a variable name (or a &lt;code&gt;_&lt;/code&gt; discard). This is no longer required with C# 9:&lt;/p&gt; &lt;pre&gt;// is pattern with Type if (input is Person) ... // case pattern with Type switch (input) { case Person: ... // is pattern with tuple Type if (input is (int, string)) ... &lt;/pre&gt; &lt;h2&gt;Combining patterns&lt;/h2&gt; &lt;p&gt;With the &lt;code&gt;is&lt;/code&gt; expression in earlier versions of C#, you could already combine patterns using regular logical operators:&lt;/p&gt; &lt;pre&gt;if (person is Student || person is Teacher) ... &lt;/pre&gt; &lt;p&gt;However, this doesn’t work for &lt;code&gt;switch&lt;/code&gt; expressions and &lt;code&gt;switch&lt;/code&gt; &lt;code&gt;case&lt;/code&gt; labels. C# 9 adds support for combining patterns using the &lt;code&gt;and&lt;/code&gt; and &lt;code&gt;or&lt;/code&gt; keywords, which works for both &lt;code&gt;if&lt;/code&gt; and &lt;code&gt;switch&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;if (person is Student or Teacher) ... decimal discount = person switch { Student or Teacher =&amp;#62; 0.1m, _ =&amp;#62; 0 }; switch (person) { case Student or Teacher: ... &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;and&lt;/code&gt; patterns have higher precedence than the &lt;code&gt;or&lt;/code&gt; patterns. You can add parentheses to change or clarify the precedence.&lt;/p&gt; &lt;h2&gt;Inverting patterns&lt;/h2&gt; &lt;p&gt;With C# 9, you can invert patterns using the &lt;code&gt;not&lt;/code&gt; keyword:&lt;/p&gt; &lt;pre&gt;if (person is not Student) ... switch (person) { case not Student: ... &lt;/pre&gt; &lt;p&gt;An interesting case is the &lt;code&gt;is not null&lt;/code&gt; pattern. This will check whether the reference is not null. Using &lt;code&gt;!= null&lt;/code&gt; may check something different when the type overloads the &lt;code&gt;!=&lt;/code&gt; operator.&lt;/p&gt; &lt;pre&gt;if (person is not null) ... &lt;/pre&gt; &lt;h2&gt;Relational patterns&lt;/h2&gt; &lt;p&gt;Relational patterns allow you to compare an expression to a constant numeric value:&lt;/p&gt; &lt;pre&gt;decimal discount = age switch { &amp;#60;= 2 =&amp;#62; 1, &amp;#60; 6 =&amp;#62; 0.5m, &amp;#60; 10 =&amp;#62; 0.2m, _ =&amp;#62; 0 }; &lt;/pre&gt; &lt;h2&gt;Patterns within patterns&lt;/h2&gt; &lt;p&gt;Patterns can also contain other patterns. This nesting lets you express complex conditions in a concise and readable way. The following example combines several types of patterns:&lt;/p&gt; &lt;pre&gt;if (person is Student { Age : &amp;#62;20 and &amp;#60;30 }) ... &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we looked at the new pattern matching features in C# 9. The additions allow you to express more complex conditions with a clear, concise syntax.&lt;/p&gt; &lt;p&gt;C# 9 can be used with the &lt;a target="_blank" rel="nofollow" href="/blog/2020/12/22/net-5-0-now-available-for-red-hat-enterprise-linux-and-red-hat-openshift/"&gt;.NET 5 SDK&lt;/a&gt;, which is available on&lt;br /&gt; &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, on &lt;a target="_blank" rel="nofollow" href="http://fedoraloves.net/"&gt;Fedora&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://dotnet.microsoft.com/download"&gt;from Microsoft for Windows, macOS, and other Linux distributions&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#38;linkname=C%23%209%20pattern%20matching" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fc-9-pattern-matching%2F&amp;#038;title=C%23%209%20pattern%20matching" data-a2a-url="https://developers.redhat.com/blog/2021/04/06/c-9-pattern-matching/" data-a2a-title="C# 9 pattern matching"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/06/c-9-pattern-matching/"&gt;C# 9 pattern matching&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/O0b7SuuZc4k" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The previous article in our C# 9 series looked at top-level programs and target-typed expressions. In this article, we’ll cover new features for pattern matching. You can find an overview of the syntax offered by previous versions of C# in C# 8 pattern matching. Type patterns When checking against a type, previous versions of C# required [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/06/c-9-pattern-matching/"&gt;C# 9 pattern matching&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/06/c-9-pattern-matching/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">1</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">873797</post-id><dc:creator>Tom Deseyn</dc:creator><dc:date>2021-04-06T07:00:57Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/06/c-9-pattern-matching/</feedburner:origLink></entry><entry><title>Get started with clang-tidy in Red Hat Enterprise Linux</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3fYHvEljRIY/" /><category term="C++" /><category term="CI/CD" /><category term="clang/LLVM" /><category term="Linux" /><category term="Security" /><category term="clang-tidy" /><category term="CMake" /><category term="code analysis" /><category term="debugging" /><category term="RHEL" /><author><name>Tom Stellard</name></author><id>https://developers.redhat.com/blog/?p=833357</id><updated>2021-04-06T07:00:23Z</updated><published>2021-04-06T07:00:23Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://clang.llvm.org/extra/clang-tidy/#clang-tidy"&gt;Clang-tidy&lt;/a&gt; is a standalone linter tool for checking &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C and C++&lt;/a&gt; source code files. It provides an additional set of compiler warnings—called &lt;em&gt;checks&lt;/em&gt;—that go above and beyond what is typically included in a C or C++ compiler. Clang-tidy comes with a large set of built-in checks and a framework for writing your own checks, as well.&lt;/p&gt; &lt;p&gt;Clang-tidy uses the same front-end libraries as the &lt;a target="_blank" rel="nofollow" href="https://clang.llvm.org/"&gt;Clang C language compiler&lt;/a&gt;. However, because it only takes source files as input, you can use &lt;code&gt;clang-tidy&lt;/code&gt; for any C or C++ codebase no matter what compiler you are using.&lt;/p&gt; &lt;p&gt;This article is a quick introduction to code analysis with clang-tidy, including how to check for rule violations in a simple C-based program and how to integrate clang-tidy with your build system.&lt;/p&gt; &lt;h2&gt;Using clang-tidy in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;In &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL), &lt;code&gt;clang-tidy&lt;/code&gt; is included as part of the &lt;a target="_blank" rel="nofollow" href="/blog/2017/11/01/getting-started-llvm-toolset/"&gt;LLVM toolset&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;# RHEL7 $ yum install llvm-toolset-10.0-clang-tools-extra # RHEL8 $ yum install clang-tools-extra &lt;/pre&gt; &lt;p&gt;The best way to get started with &lt;code&gt;clang-tidy&lt;/code&gt; is to review the list of included checks to see which ones might be useful for your codebase. The &lt;a target="_blank" rel="nofollow" href="https://clang.llvm.org/extra/clang-tidy/"&gt;Clang-tidy project page&lt;/a&gt; includes a summary of the different kinds of checks available. You can see a list of the individual checks available by running:&lt;/p&gt; &lt;pre&gt;$ clang-tidy -checks=* -list-checks &lt;/pre&gt; &lt;p&gt;Today, we are going to focus on the checks for the &lt;a target="_blank" rel="nofollow" href="https://wiki.sei.cmu.edu/confluence/display/c/SEI+CERT+C+Coding+Standard"&gt;SEI CERT Secure Coding Standard&lt;/a&gt;, which are denoted in &lt;code&gt;clang-tidy&lt;/code&gt; by the &lt;code&gt;cert-&lt;/code&gt; prefix.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: The SEI CERT Secure Coding Standard is maintained by the computer emergency response team (CERT) for the Software Engineering Institute (SEI).&lt;/p&gt; &lt;h2&gt;Checking for errors with clang-tidy&lt;/h2&gt; &lt;p&gt;The following example program violates two of the CERT Secure Coding Standard rules, &lt;a target="_blank" rel="nofollow" href="https://wiki.sei.cmu.edu/confluence/pages/viewpage.action?pageId=87152177"&gt;ENV33-C&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://wiki.sei.cmu.edu/confluence/display/c/ERR34-C.+Detect+errors+when+converting+a+string+to+a+number"&gt;ERR34-C&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;stdlib.h&amp;#62; int string_to_int(const char *num) { return atoi(num); } void ls() { system("ls"); } &lt;/pre&gt; &lt;p&gt;Let&amp;#8217;s see what happens when we run &lt;code&gt;clang-tidy&lt;/code&gt; on this code:&lt;/p&gt; &lt;pre&gt;$ clang-tidy -checks=cert-* -warnings-as-errors=* cert-err.c 2 warnings generated. cert-err.c:4:10: error: 'atoi' used to convert a string to an integer value, but function will not report conversion errors; consider using 'strtol' instead [cert-err34-c,-warnings-as-errors] return atoi(num); ^ cert-err.c:8:3: error: calling 'system' uses a command processor [cert-env33-c,-warnings-as-errors] system("ls"); ^ &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-checks=cert-*&lt;/code&gt; option tells &lt;code&gt;clang-tidy&lt;/code&gt; to enable all of the CERT Secure Coding Standard checks, and the &lt;code&gt;-warnings-as-errors=*&lt;/code&gt; option tells it to treat all warnings as errors. The &lt;code&gt;-warnings-as-errors&lt;/code&gt; takes a wildcard argument, so you can choose which warnings to promote to errors. For example, if you wanted to enable all checks, but only generate an error on the CERT checks, you could do this:&lt;/p&gt; &lt;pre&gt;$ clang-tidy -checks=* -warnings-as-errors=cert-* cert-err.c &lt;/pre&gt; &lt;h2&gt;Integrate clang-tidy into your build system&lt;/h2&gt; &lt;p&gt;In addition to manually running &lt;code&gt;clang-tidy&lt;/code&gt; on your source files, you can also integrate the tool into your build system. Build integration makes it easier to automate the checks and include them in a &lt;a target="_blank" rel="nofollow" href="/topics/ci-cd"&gt;continuous integration (CI) system&lt;/a&gt;. A simple way to integrate &lt;code&gt;clang-tidy&lt;/code&gt; into your build is to run the checks as part of a &lt;code&gt;check&lt;/code&gt; target, using &lt;code&gt;make&lt;/code&gt; or a similar build tool.&lt;/p&gt; &lt;p&gt;Looking back at our previous example, we can construct a simple makefile with a &lt;code&gt;clang-tidy&lt;/code&gt; integration:&lt;/p&gt; &lt;pre&gt;SOURCES=cert-err.c OBJS=cert-err.o all: $(OBJS) %.o: %.c $(CC) -c -o $@ $&amp;#60; $(CPPFLAGS) $(CFLAGS) check: $(SOURCES) clang-tidy $(CPPFLAGS) -checks=cert-* --warnings-as-errors=* $(SOURCES) &lt;/pre&gt; &lt;p&gt;Now, we can run our&lt;code&gt;clang-tidy&lt;/code&gt; checks using &lt;code&gt;make check&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Using a compilation database&lt;/h2&gt; &lt;p&gt;If you are using a &lt;a target="_blank" rel="nofollow" href="https://cmake.org/"&gt;CMake&lt;/a&gt; base build system, &lt;code&gt;clang-tidy&lt;/code&gt; can employ a compilation database. That way, you don&amp;#8217;t need to manually pass the same &lt;code&gt;CPPFLAGS&lt;/code&gt; you used while compiling. To generate the compilation database, you just need to pass the &lt;code&gt;-DCMAKE_EXPORT_COMPILE_COMMANDS=ON&lt;/code&gt; option to CMake when configuring. Here&amp;#8217;s an example CMake configuration for using the compilation database:&lt;/p&gt; &lt;pre&gt;set(sources ${CMAKE_SOURCE_DIR}/cert-err.c) add_library(cert-err ${sources}) add_custom_target( clang-tidy-check clang-tidy -p ${CMAKE_BINARY_DIR}/compile_commands.json -checks=cert* ${sources} DEPENDS ${sources}) add_custom_target(check DEPENDS clang-tidy-check) &lt;/pre&gt; &lt;p&gt;When you configure with CMake, it will generate a file called &lt;code&gt;compile_commands.json&lt;/code&gt;, which &lt;code&gt;clang-tidy&lt;/code&gt; uses to determine which compiler flags to employ:&lt;/p&gt; &lt;pre&gt;$ cmake . -DCMAKE_EXPORT_COMPILE_COMMANDS=ON $ make clang-tidy-check &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This was a basic introduction to &lt;code&gt;clang-tidy&lt;/code&gt;, but there is much more that you can do with it. For more information, read the upstream &lt;a target="_blank" rel="nofollow" href="https://clang.llvm.org/extra/clang-tidy/"&gt;Clang-tidy project page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#38;linkname=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F06%2Fget-started-with-clang-tidy-in-red-hat-enterprise-linux%2F&amp;#038;title=Get%20started%20with%20clang-tidy%20in%20Red%20Hat%20Enterprise%20Linux" data-a2a-url="https://developers.redhat.com/blog/2021/04/06/get-started-with-clang-tidy-in-red-hat-enterprise-linux/" data-a2a-title="Get started with clang-tidy in Red Hat Enterprise Linux"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/06/get-started-with-clang-tidy-in-red-hat-enterprise-linux/"&gt;Get started with clang-tidy in Red Hat Enterprise Linux&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3fYHvEljRIY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Clang-tidy is a standalone linter tool for checking C and C++ source code files. It provides an additional set of compiler warnings—called checks—that go above and beyond what is typically included in a C or C++ compiler. Clang-tidy comes with a large set of built-in checks and a framework for writing your own checks, as [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/06/get-started-with-clang-tidy-in-red-hat-enterprise-linux/"&gt;Get started with clang-tidy in Red Hat Enterprise Linux&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/06/get-started-with-clang-tidy-in-red-hat-enterprise-linux/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">833357</post-id><dc:creator>Tom Stellard</dc:creator><dc:date>2021-04-06T07:00:23Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/06/get-started-with-clang-tidy-in-red-hat-enterprise-linux/</feedburner:origLink></entry><entry><title>Testing Apicurio Registry’s performance and scalability</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Pp1MZfjtV3g/" /><category term="Integration" /><category term="Kubernetes" /><category term="Performance" /><category term="Python" /><category term="Apache Avro" /><category term="Apache Kafka" /><category term="Apicurio" /><category term="performance testing" /><author><name>Jan Hutař</name></author><id>https://developers.redhat.com/blog/?p=844077</id><updated>2021-04-05T07:00:56Z</updated><published>2021-04-05T07:00:56Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/registry/"&gt;Apicurio Registry&lt;/a&gt; is the upstream project for &lt;a target="_blank" rel="nofollow" href="/integration"&gt;Red Hat Integration&lt;/a&gt;’s Service Registry component. Developers use Apicurio Registry to manage artifacts like &lt;a target="_blank" rel="nofollow" href="/topics/api-management"&gt;API definitions&lt;/a&gt; and data structure schemas.&lt;/p&gt; &lt;p&gt;Apicurio Registry can maintain tons of artifacts, and it needs a way to store them. The registry supports several storage options, including &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;, Infinispan, and PostgreSQL. Knowing the performance characteristics of each storage option helps developers choose the appropriate storage for different use cases.&lt;/p&gt; &lt;p&gt;Recently, Red Hat&amp;#8217;s Performance &amp;#38; Scale team analyzed how Apicurio Registry performs under various storage configurations. In this article, we share the results of our performance and scalability testing on Apicurio Registry.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: See &lt;a target="_blank" rel="nofollow" href="/blog/2020/12/09/new-features-and-storage-options-in-red-hat-integration-service-registry-1-1-ga/"&gt;&lt;em&gt;New features and storage options in Red Hat Integration Service Registry 1.1 GA&lt;/em&gt;&lt;/a&gt; for more about Red Hat Integration and the Service Registry component.&lt;/p&gt; &lt;h2&gt;Overview of Apicurio Registry and the test setup&lt;/h2&gt; &lt;p&gt;Apicurio Registry manages artifacts such as API definitions or data structure schemas like &lt;a target="_blank" rel="nofollow" href="http://avro.apache.org/docs/current/"&gt;Apache Avro&lt;/a&gt;, which we used for these tests. As a developer, you can use API definitions and data structure schemas across your asynchronous messaging applications to validate the messages they&amp;#8217;re producing and consuming. Apicurio Registry helps you decouple the structure of your data from your applications.&lt;/p&gt; &lt;p&gt;Figure 1 shows a typical workflow with Apicurio Registry and Kafka.&lt;/p&gt; &lt;div id="attachment_844097" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/12/2020-12-15-Schema-registry-principle-1.png"&gt;&lt;img aria-describedby="caption-attachment-844097" class="wp-image-844097" src="https://developers.redhat.com/blog/wp-content/uploads/2020/12/2020-12-15-Schema-registry-principle-1.png" alt="A diagram of a typical Apicurio Registry workflow with Kafka." width="640" height="243" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/12/2020-12-15-Schema-registry-principle-1.png 814w, https://developers.redhat.com/blog/wp-content/uploads/2020/12/2020-12-15-Schema-registry-principle-1-300x114.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/12/2020-12-15-Schema-registry-principle-1-768x292.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-844097" class="wp-caption-text"&gt;Figure 1: A typical application workflow using Apache Kafka and Apicurio Registry.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The most common operation within a schema registry is a simple &lt;code&gt;GET&lt;/code&gt; request to its API to retrieve a given schema&amp;#8217;s latest version. Changing or updating the schema happens less frequently. As a result, the calls we used in our testing are fairly simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;List all of the artifacts: &lt;code&gt;GET &amp;#60;registry_host&amp;#62;/api/artifacts&lt;/code&gt;. (Note that there is no pagination. Use &lt;code&gt;search&lt;/code&gt; if needed.)&lt;/li&gt; &lt;li&gt;Get the latest version of a schema: &lt;code&gt;GET &amp;#60;registry_host&amp;#62;/api/artifacts/&amp;#60;artifact_id&amp;#62;&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Create a new schema with JSON data: &lt;code&gt;POST &amp;#60;registry_host&amp;#62;/api/artifacts&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Add a new version of a schema with JSON data: &lt;code&gt;PUT &amp;#60;registry_host&amp;#62;/api/artifacts/&amp;#60;artifact_id&amp;#62;&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Delete a schema: &lt;code&gt;DELETE &amp;#60;registry_host&amp;#62;/api/artifacts/&amp;#60;artifact_id&amp;#62;&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: When using Apache Kafka to transfer Avro messages, the default Apicurio Registry client libraries don&amp;#8217;t load the Avro schema on every request. They only load schemas on application startup (or, for consumers, when a schema changes), so registry performance does not affect the speed of producing and consuming messages.&lt;/p&gt; &lt;h2&gt;Performance testing Apicurio Registry&lt;/h2&gt; &lt;p&gt;Our performance tests were basic, but each step contained multiple variants to catch various Apicurio Registry configurations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clean up the registry database for a clean starting point.&lt;/li&gt; &lt;li&gt;Populate the registry with a given number of schemas.&lt;/li&gt; &lt;li&gt;Flood the registry with &lt;code&gt;GET&lt;/code&gt; requests for the latest version using random schema from those created in the previous step.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;How we tested&lt;/h3&gt; &lt;p&gt;We used a Python script to generate a load of &lt;code&gt;GET&lt;/code&gt;s to the registry, and we used &lt;a target="_blank" rel="nofollow" href="https://locust.io/"&gt;Locust&lt;/a&gt; as our load testing tool. This setup might be overkill for our use case, where we&amp;#8217;re calling just one endpoint with a random schema ID, but it is a good test setup in general.&lt;/p&gt; &lt;p&gt;We use Locust as a library in our custom &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-performance/opl/blob/master/opl/locust.py"&gt;locust.py tool&lt;/a&gt;. Our custom tool has the added benefit of generating JSON files with the results and additional data that you can easily analyze later. Using Locust&amp;#8217;s default command-line interface tool would also work here.&lt;/p&gt; &lt;p&gt;Our deployment environment was &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift 4&lt;/a&gt; cluster running on Amazon Web Services Elastic Compute Cloud. We conducted some of our tests using an installation created by an Apicurio Registry Operator; other tests were conducted as custom deployment configurations for more control. Both the PostgreSQL database and load generation scripts could run in a pod in the same cluster. To monitor our pods, we used data from OpenShift’s Prometheus in the &lt;code&gt;openshift-monitoring&lt;/code&gt; namespace.&lt;/p&gt; &lt;h3&gt;Scaling the Locust script horizontally&lt;/h3&gt; &lt;p&gt;Scaling the Locust script was one of the issues we had to solve during testing. When we raised the registry pod&amp;#8217;s CPU resources, we noticed an upper limit of about 925 requests per second. This indicated the application was scaling past two CPUs, which was unexpected. When we monitored the data, it didn&amp;#8217;t indicate that the resources were saturated on the registry or on the database, so we scaled the test script horizontally to distribute the load to more pods. When we scaled the script horizontally, we were able to generate many more requests.&lt;/p&gt; &lt;p&gt;Figure 2 shows the flow for scaling the Locust script horizontally.&lt;/p&gt; &lt;div id="attachment_886477" style="width: 649px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-886477" class="wp-image-886477" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/2021-03-25-Locust-principle.png" alt="Horizontal scaling of the Locust script allows for more requests to system under the test." width="639" height="345" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/2021-03-25-Locust-principle.png 837w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/2021-03-25-Locust-principle-300x162.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/2021-03-25-Locust-principle-768x415.png 768w" sizes="(max-width: 639px) 100vw, 639px" /&gt;&lt;p id="caption-attachment-886477" class="wp-caption-text"&gt;Figure 2: Horizontally scaling the Locust script allows for more requests to the system under test.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 3 shows the requests per second (RPS) for different CPU resources with one Locust follower node.&lt;/p&gt; &lt;div id="attachment_844117" style="width: 610px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-844117" class="wp-image-844117 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/12/RPS-for-different-CPU-resources-with-1300Mi-PostgreSQL.png" alt="A graph showing RPS for CPU resources on PostgreSQL." width="600" height="371" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/12/RPS-for-different-CPU-resources-with-1300Mi-PostgreSQL.png 600w, https://developers.redhat.com/blog/wp-content/uploads/2020/12/RPS-for-different-CPU-resources-with-1300Mi-PostgreSQL-300x186.png 300w" sizes="(max-width: 600px) 100vw, 600px" /&gt;&lt;p id="caption-attachment-844117" class="wp-caption-text"&gt;Figure 3: Requests per second for different CPU resources with one Locust follower node.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 4 shows the requests per second after scaling for 10 Locust follower nodes.&lt;/p&gt; &lt;div id="attachment_844107" style="width: 610px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-844107" class="wp-image-844107 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/12/RPS-for-different-CPU-resources-with-1300Mi-PostgreSQL1.png" alt="A graph showing RPS for CPU resources, scaled from one Locust follower node to 10." width="600" height="371" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/12/RPS-for-different-CPU-resources-with-1300Mi-PostgreSQL1.png 600w, https://developers.redhat.com/blog/wp-content/uploads/2020/12/RPS-for-different-CPU-resources-with-1300Mi-PostgreSQL1-300x186.png 300w" sizes="(max-width: 600px) 100vw, 600px" /&gt;&lt;p id="caption-attachment-844107" class="wp-caption-text"&gt;Figure 4: Requests per second for different CPU resources with 10 Locust follower nodes.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;We found the following results from testing Apicurio Registry&amp;#8217;s performance and scalability:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apicurio Registry works consistently and isn&amp;#8217;t affected by the number of artifacts it has in the database. We tested the registry with one million schemas, each with 10 versions and each version with 345 bytes of serialized JSON on average.&lt;/li&gt; &lt;li&gt;Apicurio Registry&amp;#8217;s performance grows linearly as we allocate more CPU resources to it.&lt;/li&gt; &lt;li&gt;Apicurio Registry&amp;#8217;s performance grows linearly as more registry pods are started. We tested 10 pods, which provided schemas at a rate of 4,201 requests per second.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We conclude that Apicurio Registry is capable of handling a wide range of deployments. We can always test more, but the current results show that Apicurio Registry with a PostgreSQL storage backend is a good option for future deployments.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#38;linkname=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F05%2Ftesting-apicurio-registrys-performance-and-scalability%2F&amp;#038;title=Testing%20Apicurio%20Registry%E2%80%99s%20performance%20and%20scalability" data-a2a-url="https://developers.redhat.com/blog/2021/04/05/testing-apicurio-registrys-performance-and-scalability/" data-a2a-title="Testing Apicurio Registry’s performance and scalability"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/05/testing-apicurio-registrys-performance-and-scalability/"&gt;Testing Apicurio Registry&amp;#8217;s performance and scalability&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Pp1MZfjtV3g" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Apicurio Registry is the upstream project for Red Hat Integration’s Service Registry component. Developers use Apicurio Registry to manage artifacts like API definitions and data structure schemas. Apicurio Registry can maintain tons of artifacts, and it needs a way to store them. The registry supports several storage options, including Apache Kafka, Infinispan, and PostgreSQL. Knowing [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/05/testing-apicurio-registrys-performance-and-scalability/"&gt;Testing Apicurio Registry&amp;#8217;s performance and scalability&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/05/testing-apicurio-registrys-performance-and-scalability/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">844077</post-id><dc:creator>Jan Hutař</dc:creator><dc:date>2021-04-05T07:00:56Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/05/testing-apicurio-registrys-performance-and-scalability/</feedburner:origLink></entry><entry><title type="html">Headless eCommerce - An architectural introduction</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MfrpyMiDgSg/headless-ecommerce-an-architectural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/9Rniyo9DMGU/headless-ecommerce-an-architectural-introduction.html</id><updated>2021-04-05T05:00:00Z</updated><content type="html">Part 1 - An architectural introduction We're kicking off another series sharing a new architecture blueprint. It's focusing on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge creating architectural content based on common customer adoption patterns. That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Architectural Blueprint.  Let's look at these blueprints, how their created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division in to smaller parts. In this case we've aligned with the higher level theme being 'Retail' use cases, a vertical focus. This breaks down into the following use cases and in no particular order: * * * * * Store health and safety * Real-time stock control * Retail data framework The case I'm tackling here is focused on Headless eCommerce. This use case we've defined as the following: Deploying a container based eCommerce website while moving away from tightly coupled existing eCommerce platform. The approach taken is to research our existing customers that have implemented solutions in this space, collect their public facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  To get an idea of what these blueprints look like, we refer you to the series previously discussed here: * * * * Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architectural blueprint for use telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on headless e-commerce portfolio architecture blueprint: 1. 2. Common architectural elements 3. Example headless architectures Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at the generic common architectural elements for a headless e-commerce architecture. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MfrpyMiDgSg" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/9Rniyo9DMGU/headless-ecommerce-an-architectural-introduction.html</feedburner:origLink></entry><entry><title type="html">JAX-RS ParamConverter with Quarkus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/a86mWcHaEMM/" /><author><name /></author><id>https://resteasy.github.io/2021/04/05/blog-ParamConverter-with-Quarkus/</id><updated>2021-04-05T00:00:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/a86mWcHaEMM" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/04/05/blog-ParamConverter-with-Quarkus/</feedburner:origLink></entry><entry><title>Design event-driven integrations with Kamelets and Camel K</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fYcxGoMh7mI/" /><category term="Event-Driven" /><category term="Kubernetes" /><category term="Microservices" /><category term="Serverless" /><category term="Camel K" /><category term="Camel K integration" /><category term="integration developer" /><category term="kamelet" /><category term="YAML DSL" /><author><name>Pasquale Congiusti</name></author><id>https://developers.redhat.com/blog/?p=854707</id><updated>2021-04-02T07:00:58Z</updated><published>2021-04-02T07:00:58Z</published><content type="html">&lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/kamelet-component.html"&gt;Kamelet&lt;/a&gt; is a concept introduced near the end of 2020 by &lt;a target="_blank" rel="nofollow" href="/topics/camel-k"&gt;Camel K&lt;/a&gt; to simplify the design of complex system integrations. If you’re familiar with the &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/"&gt;Camel&lt;/a&gt; ecosystem, you know that Camel offers many &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/"&gt;components&lt;/a&gt; for integrating existing systems. An integration&amp;#8217;s granularity is related to its low-level components, however. With Kamelets you can reason at a higher level of abstraction than with Camel alone.&lt;/p&gt; &lt;p&gt;A &lt;i&gt;Kamelet&lt;/i&gt; is a document specifying an integration flow. A Kamelet uses Camel components and enterprise integration patterns (EIPs) to describe a system&amp;#8217;s behavior. You can reuse a Kamelet abstraction in any integration on top of a &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. You can use any of Camel&amp;#8217;s domain-specific languages (DSLs) to write a Kamelet, but the most natural choice is the YAML DSL. The YAML DSL is designed to specify input and output formats, so any integration developer knows beforehand what kind of data to expect.&lt;/p&gt; &lt;p&gt;A Kamelet also serves as a source or sink of events, making Kamelets the building blocks of an &lt;a target="_blank" rel="nofollow" href="/topics/event-driven"&gt;event-driven architecture&lt;/a&gt;. Later in the article, I will introduce the concept of source and sink events and how to use them in an event-driven architecture with Kamelets.&lt;/p&gt; &lt;p&gt;&lt;span id="more-854707"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Capturing system-complex behavior&lt;/h2&gt; &lt;p&gt;In the pre-Kamelet era, you would typically design an enterprise integration by reasoning about its lower-level components. An integration would be composed of queues, APIs, and databases glued together to produce or consume an event. Ideally, you would use Camel EIPs and components to develop the integration, and Camel K to run it on Kubernetes.&lt;/p&gt; &lt;p&gt;With the advent of Kamelets, you can think of the design phase as a &lt;em&gt;declarative phase&lt;/em&gt;, where you describe the flow in a YAML document. You simply need to describe your system specifications. Most of the time, you will define variables that are provided at runtime, just before running the integration. Eventually, you will use visual tools to help with features such as suggestions and autocomplete. Once the Kamelet is ready, you can install its definition on a Kubernetes cluster and use it in a real integration.&lt;/p&gt; &lt;p&gt;At this stage, you still haven&amp;#8217;t executed anything. To see a Kamelet in action, you must use it inside an integration, likely specifying concrete values for the expected variables. I think this is Kamelet&amp;#8217;s most remarkable feature. It splits integration development into two phases: design (abstract) and execution (concrete).&lt;/p&gt; &lt;p&gt;With this approach, you can provide to your integration developers a set of prebuilt, high-level abstractions to use in their concrete integrations. You can create, for example, a Kamelet whose scope is to expose events of a certain enterprise domain. You can think of each Kamelet as an encapsulation of anything with a certain “business” complexity. You can use these building blocks in any refined integration with less hassle.&lt;/p&gt; &lt;p&gt;Basically, we can view a Kamelet as a distilled view of a complex system exposing a limited set of information. The integration uses this information in its execution flow.&lt;/p&gt; &lt;h2&gt;Kamelets in a Camel integration&lt;/h2&gt; &lt;p&gt;Using Kamelets in a real integration is straightforward. You can think of Kamelets as a new Camel component. Each Kamelet can be designed as either a source (&lt;i&gt;from&lt;/i&gt; in the Camel DSL) or a sink (&lt;i&gt;to&lt;/i&gt; in the Camel DSL). (We will talk more about designing Kamelets as sources and sinks soon.)&lt;/p&gt; &lt;p&gt;A Camel K Operator can spot the &lt;code&gt;kamelet&lt;/code&gt; component handling all the magic behind the scenes. So, as soon as you run your integration, the Kamelet immediately starts doing its job.&lt;/p&gt; &lt;h3&gt;Kamelets in action&lt;/h3&gt; &lt;p&gt;A demonstration is usually worth a thousand words, so let’s see a real example. First of all, we’ll define a Kamelet. For this example, we are going to enjoy a fresh beer:&lt;/p&gt; &lt;pre&gt;apiVersion: camel.apache.org/v1alpha1 kind: Kamelet metadata: name: beer-source labels: camel.apache.org/kamelet.type: "source" spec: definition: title: "Beer Source" description: "Retrieve a random beer from catalog" properties: period: title: Period description: The interval between two events type: integer default: 1000 types: out: mediaType: application/json flow: from: uri: timer:tick parameters: period: "#property:period" steps: - to: "https://random-data-api.com/api/beer/random_beer" - to: "kamelet:sink" &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;flow&lt;/code&gt; section could be much more elaborate depending on the complexity of the abstraction you want to design. Let’s call this file &lt;code&gt;beer-source.kamelet.yaml&lt;/code&gt;. We can now create this Kamelet on our Kubernetes cluster:&lt;/p&gt; &lt;pre&gt;$ kubectl apply -f beer-source.kamelet.yaml &lt;/pre&gt; &lt;p&gt;In this case, we’ve used the Kubernetes command-line interface (CLI) because we’re just declaring the kind of Kamelet. Using &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;&amp;#8216;s &lt;code&gt;oc&lt;/code&gt; would also be fine. To see the list of available Kamelets on the cluster, let&amp;#8217;s run:&lt;/p&gt; &lt;pre&gt;$ kubectl get kamelets NAME           PHASE beer-source    Ready &lt;/pre&gt; &lt;p&gt;We’ll use it in a real integration by replacing the previous &lt;code&gt;flow&lt;/code&gt; section with the following:&lt;/p&gt; &lt;pre&gt;- from: uri: "kamelet:beer-source?period=5000" steps: - log: "${body}" &lt;/pre&gt; &lt;p&gt;Now, we have a new component named &lt;code&gt;kamelet&lt;/code&gt;. We’re creating a new integration from &lt;code&gt;beer-source&lt;/code&gt;, which is the Kamelet name we’ve provided. We’re also setting a variable, &lt;code&gt;period&lt;/code&gt;, that we’ve previously declared in the Kamelet.&lt;/p&gt; &lt;h3&gt;Run the Kamelet&lt;/h3&gt; &lt;p&gt;Camel K is the easiest way to run a Kamelet. If you don’t yet have Camel K installed, follow the &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/installation/installation.html"&gt;instructions to install it&lt;/a&gt;. When you are ready, bundle the integration in a file (say, &lt;code&gt;beers.yaml&lt;/code&gt;) and run:&lt;/p&gt; &lt;pre&gt;$ kamel run beers.yaml --dev &lt;/pre&gt; &lt;p&gt;As soon as you run the integration, you will start enjoying fresh beers (just on screen) every five seconds.&lt;/p&gt; &lt;h2&gt;Additional integration features&lt;/h2&gt; &lt;p&gt;Now that you&amp;#8217;ve seen a demonstration let&amp;#8217;s get back to theory. Kamelets support type specification and reusability, which are both useful features for complex integrations.&lt;/p&gt; &lt;h3&gt;Type specification&lt;/h3&gt; &lt;p&gt;An important aspect of using Kamelets is &lt;em&gt;type specification&lt;/em&gt;. In a Kamelet, you can declare the kind of data type that the Kamelet will consume as input and produce as output. This specification is very useful if the integration developer does not want to deal with the complex logic the Kamelet is hiding. Type information is available as a JSON schema, so a developer can easily read it and understand the information it exposes.&lt;/p&gt; &lt;p&gt;Let’s imagine a Kamelet designed to extract the prices from a catalog of products. The prices are calculated by filtering, splitting, and transforming data in a system composed of various endpoints and databases. At the end of the day, a developer wants to know what data format to expect. Having a name, description, and data types gives the developer a fair idea of what to expect from the Kamelet.&lt;/p&gt; &lt;h3&gt;Reusability&lt;/h3&gt; &lt;p&gt;I mentioned that a Kamelet is installed on a Kubernetes cluster, where it is ready to be used in any Camel K integration. The installation procedure is an extension of Kubernetes via a custom resource endpoint.&lt;/p&gt; &lt;p&gt;This is a nice way to make Kamelets first-class citizens of the cluster, letting them be discovered and used by other applications. Having a variety of Kamelets that others in your organization can reuse means you won’t need to reinvent the wheel if something is already there.&lt;/p&gt; &lt;p&gt;The possibility of defining variables as abstract placeholders for data that can change is key to reusability. If you can define a general flow that describes an integration or a part of an integration with parameters, you will have a Kamelet ready for possible reuse.&lt;/p&gt; &lt;h2&gt;Using Kamelets in an event-driven architecture&lt;/h2&gt; &lt;p&gt;Because Kamelets are high-level concepts that define input and output, it’s possible to think of them as the building blocks of an event-driven architecture. Earlier, I mentioned that you could create a Kamelet as a source or sink. In an event-driven architecture, a &lt;em&gt;source&lt;/em&gt; emits events, which a sink can consume. Therefore, you can think of a system that &lt;em&gt;produces&lt;/em&gt; events (a source) or &lt;em&gt;consumes&lt;/em&gt; events (a sink). The idea is to distill a view of a more complex system.&lt;/p&gt; &lt;h3&gt;Declaring high-level flows in a Kamelet binding&lt;/h3&gt; &lt;p&gt;Camel K engineers have put together these concepts to include a middle layer in charge of routing events to the interested parties. This is done declaratively in a &lt;em&gt;Kamelet binding&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;A Kamelet binding lets you tell a Kamelet source to send events to a destination that can be represented by a &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture"&gt;Knative&lt;/a&gt; channel, a &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt; topic, or a generic URI. The same mechanism lets a Kamelet sink consume events from a destination. Thus, you can use Kamelet bindings to define the high-level flows of an event-driven architecture, as shown in Figure 1. The logic of routing is completely hidden from the final user.&lt;/p&gt; &lt;div id="attachment_859357" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/eda_jabberwocky.png"&gt;&lt;img aria-describedby="caption-attachment-859357" class="wp-image-859357" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/eda_jabberwocky.png" alt="A diagram of high-level flows in an event-driven architecture." width="640" height="316" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/eda_jabberwocky.png 711w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/eda_jabberwocky-300x148.png 300w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-859357" class="wp-caption-text"&gt;Figure 1: Kamelet&amp;#8217;s event-driven architecture.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;An integration developer can create a workflow based on the relevant events by binding these events to a Kamelet sink. Imagine an online store. An order system emits events that can be captured by a shipping system, an inventory system, or a user preference system. The three sinks work independently, relying on this framework. Each of them watches and reacts to events.&lt;/p&gt; &lt;p&gt;The order system could be represented by the &lt;em&gt;order source Kamelet&lt;/em&gt;, which hides its internal complexity. Then, you create a Kamelet binding to start emitting events into a Knative, Kafka, or URI channel. On the producer side, a developer could design a decoupled &lt;em&gt;shipping sink Kamelet&lt;/em&gt; and bind it to the &lt;em&gt;order event channel&lt;/em&gt; via a Kamelet binding. The shipping sink will therefore start receiving events and executing the business logic that was coded into it.&lt;/p&gt; &lt;h3&gt;Kamelet bindings in action&lt;/h3&gt; &lt;p&gt;Let’s use the previous Kamelet to emit events (random beers) into a Kafka topic. To make this example work, you might need to have a running Kafka cluster. If you don&amp;#8217;t have a Kafka cluster, you can set one up with &lt;a target="_blank" rel="nofollow" href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;. Once it&amp;#8217;s ready, create a Kafka topic to use as your destination channel:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: beer-events labels: strimzi.io/cluster: my-cluster spec: partitions: 1 replicas: 1 config: retention.ms: 7200000 segment.bytes: 1073741824 &lt;/pre&gt; &lt;p&gt;Let’s call this file &lt;code&gt;beer-destination.yaml&lt;/code&gt;. Now, create the Kubernetes topic on the cluster:&lt;/p&gt; &lt;pre&gt;$ kubectl apply -f kafka-topic.yaml &lt;/pre&gt; &lt;p&gt;As soon as that is ready, declare a &lt;code&gt;KameletBinding&lt;/code&gt; whose goal is to forward any event generated by the producer to a destination:&lt;/p&gt; &lt;pre&gt;apiVersion: camel.apache.org/v1alpha1 kind: KameletBinding metadata: name: beer-event-source spec: source: ref: kind: Kamelet apiVersion: camel.apache.org/v1alpha1 name: beer-source properties: period: 5000 sink: ref: kind: KafkaTopic apiVersion: kafka.strimzi.io/v1beta1 name: beer-events &lt;/pre&gt; &lt;p&gt;Call this file &lt;code&gt;beer-source.binding.yaml&lt;/code&gt; and use it to create the resource on the cluster:&lt;/p&gt; &lt;pre&gt;$ kubectl apply -f beer-source.binding.yaml &lt;/pre&gt; &lt;h3&gt;Create the sink flow&lt;/h3&gt; &lt;p&gt;Let’s pause a moment. The structure of this document is quite simple. We declare a source and a sink. In other words, we’re telling the system where we want certain events to end up. So far, we have a source, along with a mechanism to collect its events and forward them to a destination. To continue, we need a consumer (&lt;em&gt;aka&lt;/em&gt;, a sink).&lt;/p&gt; &lt;p&gt;Let’s create a &lt;code&gt;log-sink&lt;/code&gt;. Its goal is to consume any generic event and print it out on the screen:&lt;/p&gt; &lt;pre&gt;apiVersion: camel.apache.org/v1alpha1 kind: Kamelet metadata: name: log-sink labels: camel.apache.org/kamelet.type: "sink" spec: definition: title: "Log Sink" description: "Consume events from a channel" flow: from: uri: kamelet:source steps: - to: "log:sink" &lt;/pre&gt; &lt;p&gt;This is a simple logger, but you can design a sink flow as complex as your use case requires. Let’s call this one &lt;code&gt;log-sink.kamelet.yaml&lt;/code&gt; and create the logger on our cluster:&lt;/p&gt; &lt;pre&gt;$ kubectl apply -f log-sink.kamelet.yaml &lt;/pre&gt; &lt;h3&gt;Create the Kamelet bindings&lt;/h3&gt; &lt;p&gt;Now, we create a &lt;code&gt;KameletBinding&lt;/code&gt; to match the events to the logger we just created:&lt;/p&gt; &lt;pre&gt;apiVersion: camel.apache.org/v1alpha1 kind: KameletBinding metadata: name: log-event-sink spec: source: ref: kind: KafkaTopic apiVersion: kafka.strimzi.io/v1beta1 name: beer-events sink: ref: kind: Kamelet apiVersion: camel.apache.org/v1alpha1 name: log-sink &lt;/pre&gt; &lt;p&gt;Notice that we&amp;#8217;ve just swapped the previous binding sink (&lt;code&gt;beer-event&lt;/code&gt;) to be the source, and declared the sink as the new log Kamelet. Let’s call the file &lt;code&gt;log-sink.binding.yaml&lt;/code&gt; and create it on our cluster:&lt;/p&gt; &lt;pre&gt;$ kubectl apply -f log-sink.binding.yaml &lt;/pre&gt; &lt;h3&gt;Check the running integration&lt;/h3&gt; &lt;p&gt;Behind the scenes, the Camel K &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/operators"&gt;Operator&lt;/a&gt; is in charge of transforming those Kamelet bindings into running integrations. We can see that by executing:&lt;/p&gt; &lt;pre&gt;$ kamel get NAME           PHASE    KIT beer-event-source    Running    kit-c040jsfrtu6patascmu0 log-event-sink       Running    kit-c040r3frtu6patascmv0 &lt;/pre&gt; &lt;p&gt;So, let’s have a final look at the logs to see the sink in action:&lt;/p&gt; &lt;pre&gt;$ kamel logs log-event-sink … {"id":9103,"uid":"19361426-23dd-4051-95ba-753d7f0ebe7b","brand":"Becks","name":"Alpha King Pale Ale","style":"German Wheat And Rye Beer","hop":"Mt. Rainier","yeast":"1187 - Ringwood Ale","malts":"Vienna","ibu":"39 IBU","alcohol":"3.8%","blg":"11.1°Blg"}] {"id":313,"uid":"0c64734b-2903-4a70-b0a1-bac5e10fea75","brand":"Rolling Rock","name":"Chimay Grande Réserve","style":"Dark Lager","hop":"Northern Brewer","yeast":"2565 - Kölsch","malts":"Chocolate malt","ibu":"86 IBU","alcohol":"4.8%","blg":"19.1°Blg"}] &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Thanks to the Kamelets and the Kamelet binding mechanism, we managed to create a simple event-driven architecture; a consumer decoupled from a producer. The nicest thing is that we did everything by just declaring the origin of events (&lt;code&gt;beer-source&lt;/code&gt;) and its destination (&lt;code&gt;log-sink&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#38;linkname=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F02%2Fdesign-event-driven-integrations-with-kamelets-and-camel-k%2F&amp;#038;title=Design%20event-driven%20integrations%20with%20Kamelets%20and%20Camel%20K" data-a2a-url="https://developers.redhat.com/blog/2021/04/02/design-event-driven-integrations-with-kamelets-and-camel-k/" data-a2a-title="Design event-driven integrations with Kamelets and Camel K"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/02/design-event-driven-integrations-with-kamelets-and-camel-k/"&gt;Design event-driven integrations with Kamelets and Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fYcxGoMh7mI" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The Kamelet is a concept introduced near the end of 2020 by Camel K to simplify the design of complex system integrations. If you’re familiar with the Camel ecosystem, you know that Camel offers many components for integrating existing systems. An integration&amp;#8217;s granularity is related to its low-level components, however. With Kamelets you can reason [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/02/design-event-driven-integrations-with-kamelets-and-camel-k/"&gt;Design event-driven integrations with Kamelets and Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/02/design-event-driven-integrations-with-kamelets-and-camel-k/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">2</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">854707</post-id><dc:creator>Pasquale Congiusti</dc:creator><dc:date>2021-04-02T07:00:58Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/02/design-event-driven-integrations-with-kamelets-and-camel-k/</feedburner:origLink></entry><entry><title>Deploy integration components easily with the Red Hat Integration Operator</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/sp8c7wNQAkk/" /><category term="Event-Driven" /><category term="Kubernetes" /><category term="Operator" /><category term="integration operator" /><category term="openshift" /><category term="OperatorHub" /><category term="technology integration" /><author><name>Hugo Guerrero</name></author><id>https://developers.redhat.com/blog/?p=883957</id><updated>2021-04-01T07:00:14Z</updated><published>2021-04-01T07:00:14Z</published><content type="html">&lt;p&gt;Any developer knows that when we talk about integration, we can mean many different concepts and architecture components. Integration can start with the API gateway and extend to events, data transfer, data transformation, and so on. It is easy to lose sight of what technologies are available to help you solve various business problems. &lt;a href="https://developers.redhat.com/integration/"&gt;Red Hat Integration&lt;/a&gt;&amp;#8216;s Q1 &lt;a href="https://www.redhat.com/en/blog/red-hat-delivers-new-change-data-capture-capabilities-and-enhances-user-experience-latest-red-hat-integration-release"&gt;release&lt;/a&gt; introduces a new feature that targets this challenge: the Red Hat Integration Operator.&lt;/p&gt; &lt;p&gt;The Red Hat Integration Operator helps developers easily explore and discover components available in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. One single Operator is now the entry point for getting the latest Red Hat Integration components. The Operator declaratively manages the components we want to enable, the namespaces to which we want to deploy, and their scope in the Red Hat OpenShift cluster using a &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; custom resource.&lt;/p&gt; &lt;p&gt;Now in general availability, the Red Hat Integration Operator lets you choose and install any of the following Operators:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3scale&lt;/li&gt; &lt;li&gt;3scale APIcast&lt;/li&gt; &lt;li&gt;AMQ Broker&lt;/li&gt; &lt;li&gt;AMQ Interconnect&lt;/li&gt; &lt;li&gt;AMQ Streams&lt;/li&gt; &lt;li&gt;API Designer&lt;/li&gt; &lt;li&gt;Camel K&lt;/li&gt; &lt;li&gt;Fuse Console&lt;/li&gt; &lt;li&gt;Fuse Online&lt;/li&gt; &lt;li&gt;Service Registry&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In this article, we will review the different components that the Red Hat Integration Operator manages and see how to install the Operator in OpenShift with OperatorHub. Finally, we will install 3scale APIcast and AMQ Broker Operators using a customized Install file.&lt;/p&gt; &lt;h2&gt;Installing the Red Hat Integration Operator with OperatorHub&lt;/h2&gt; &lt;p&gt;To get started using the Red Hat Integration Operator on OpenShift with &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.7/operators/understanding/olm-understanding-operatorhub.html"&gt;OperatorHub&lt;/a&gt;, follow these steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the OpenShift Container Platform web console, log in as an administrator and navigate to the &lt;b&gt;OperatorHub&lt;/b&gt;.&lt;/li&gt; &lt;li&gt;Type in &lt;code&gt;Red Hat Integration&lt;/code&gt; to filter within the available operators.&lt;/li&gt; &lt;li&gt;Select the &lt;b&gt;Red Hat Integration&lt;/b&gt; Operator.&lt;/li&gt; &lt;li&gt;Check the Operator details and click &lt;b&gt;Install&lt;/b&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;b&gt;Install Operator&lt;/b&gt; page, accept all of the default selections and click &lt;b&gt;Install&lt;/b&gt;.&lt;/li&gt; &lt;li&gt;When the install status indicates the Operator is ready to use, click &lt;b&gt;View Operator&lt;/b&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_883967" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rhi-operator-hub.png"&gt;&lt;img aria-describedby="caption-attachment-883967" class="wp-image-883967 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rhi-operator-hub-1024x494.png" alt="Locating Red Hat Integration in the OpenShift OperatorHub via a filter search" width="640" height="309" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rhi-operator-hub-1024x494.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/rhi-operator-hub-300x145.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/rhi-operator-hub-768x371.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-883967" class="wp-caption-text"&gt;Figure 1: Searching for Red Hat Integration Operator in the OpenShift OperatorHub&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now you are ready to install Operators for Red Hat Integration components. For more information on installing the Operator from OperatorHub, see the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.7/operators/admin/olm-adding-operators-to-cluster.html#olm-installing-operators-from-operatorhub_olm-adding-operators-to-a-cluster"&gt;OpenShift documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Installing Operators for Red Hat Integration components&lt;/h2&gt; &lt;p&gt;On the Operator page, you will see the Operator has provided a new &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/"&gt;custom resource definition&lt;/a&gt; (CRD) called &lt;b&gt;Installation&lt;/b&gt;. You can click &lt;b&gt;Create Instance&lt;/b&gt; to produce a new &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resource&lt;/a&gt; (CR) to trigger the Operators’ installation. The &lt;b&gt;Create Installation&lt;/b&gt; page displays the complete set of Red Hat Integration Operators available for installation. All Operators are enabled for installation by default.&lt;/p&gt; &lt;p&gt;If you prefer, you can &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q1/html/installing_the_red_hat_integration_operator_on_openshift/installing-rhi-operators#installing-integration-operator-components"&gt;configure the installation&lt;/a&gt; specification from the form or YAML views before performing the installation. Doing so lets you include or exclude Operators, change namespaces, and switch between an Operator’s supported modes.&lt;/p&gt; &lt;p&gt;Here is an example configuration for 3scale APIcast and AMQ Streams:&lt;/p&gt; &lt;pre&gt;apiVersion: integration.redhat.com/v1 kind: Installation metadata:   name: rhi-installation spec:   3scale-apicast-installation:     enabled: true      mode: namespace     namespace: rhi-3scale-apicast   amq-streams-installation:     enabled: true     mode: namespace      namespace: rhi-streams &lt;/pre&gt; &lt;p&gt;When you navigate to the &lt;b&gt;Installed Operators&lt;/b&gt; view, you should see the list of installed Operators and their managed namespaces (see Figure 2).&lt;/p&gt; &lt;div id="attachment_884007" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/isntalled_operators.png"&gt;&lt;img aria-describedby="caption-attachment-884007" class="wp-image-884007" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/isntalled_operators-1024x534.png" alt="A list of installed Operators in the OpenShift Container Platform web console" width="640" height="334" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/isntalled_operators-1024x534.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/isntalled_operators-300x157.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/isntalled_operators-768x401.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-884007" class="wp-caption-text"&gt;Figure 2: Viewing installed Operators in the OpenShift Container Platform web console&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The Red Hat Integration Operator also upgrades the Operators for the Red Hat Integration components it has installed. It defaults to an automatic approval strategy, so the components are upgraded to use the latest available versions.&lt;/p&gt; &lt;p&gt;Check the complete &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q4/html-single/installing_the_red_hat_integration_operator_on_openshift/index"&gt;documentation&lt;/a&gt; for more information about installing the Red Hat Integration Operator.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In the previous sections, we explored the different components that the Red Hat Integration Operator can install using the Installation custom resource on OpenShift. We also reviewed a simple way to install the Operator using OperatorHub and used a customized file to install only two components in their namespace scope. As you can see, it is now easier than ever to get started with Red Hat Integration on OpenShift.&lt;/p&gt; &lt;p&gt;Red Hat Integration provides an agile, distributed, and API-centric solution that organizations can use to connect and share data between applications and systems in a digital world. Get started by navigating to your OpenShift OperatorHub and installing the Red Hat Integration Operator.&lt;/p&gt; &lt;p&gt;The Red Hat Integration Operator is also available in &lt;a target="_blank" rel="nofollow" href="https://marketplace.redhat.com/en-us/products/red-hat-integration"&gt;Red Hat Marketplace&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to know more about the Red Hat Integration Operator, watch &lt;a target="_blank" rel="nofollow" href="https://youtu.be/M8756ZGmvcg"&gt;this video from Christina Lin&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#38;linkname=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fdeploy-integration-components-easily-with-the-red-hat-integration-operator%2F&amp;#038;title=Deploy%20integration%20components%20easily%20with%20the%20Red%20Hat%20Integration%20Operator" data-a2a-url="https://developers.redhat.com/blog/2021/04/01/deploy-integration-components-easily-with-the-red-hat-integration-operator/" data-a2a-title="Deploy integration components easily with the Red Hat Integration Operator"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/01/deploy-integration-components-easily-with-the-red-hat-integration-operator/"&gt;Deploy integration components easily with the Red Hat Integration Operator&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/sp8c7wNQAkk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Any developer knows that when we talk about integration, we can mean many different concepts and architecture components. Integration can start with the API gateway and extend to events, data transfer, data transformation, and so on. It is easy to lose sight of what technologies are available to help you solve various business problems. Red [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/01/deploy-integration-components-easily-with-the-red-hat-integration-operator/"&gt;Deploy integration components easily with the Red Hat Integration Operator&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/01/deploy-integration-components-easily-with-the-red-hat-integration-operator/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">883957</post-id><dc:creator>Hugo Guerrero</dc:creator><dc:date>2021-04-01T07:00:14Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/01/deploy-integration-components-easily-with-the-red-hat-integration-operator/</feedburner:origLink></entry><entry><title>Get started with XDP</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/eiVNaOvOmFg/" /><category term="C" /><category term="clang/LLVM" /><category term="Linux" /><category term="eBPF" /><category term="eXpress Data Path" /><category term="RHEL" /><category term="XDP" /><author><name>Hangbin Liu</name></author><id>https://developers.redhat.com/blog/?p=857207</id><updated>2021-04-01T07:00:13Z</updated><published>2021-04-01T07:00:13Z</published><content type="html">&lt;p&gt;XDP (eXpress Data Path) is a powerful new networking feature in Linux that enables high-performance programmable access to networking packets before they enter the networking stack. But XDP has a high learning curve. Many developers have written introduction blogs for this feature, such as Paolo Abeni&amp;#8217;s &lt;a href="https://developers.redhat.com/blog/2018/12/06/achieving-high-performance-low-latency-networking-with-xdp-part-1/"&gt;&lt;em&gt;Achieving high-performance, low-latency networking with XDP: Part I &lt;/em&gt;&lt;/a&gt;and Toke&amp;#8217;s &lt;em&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/using-express-data-path-xdp-red-hat-enterprise-linux-8"&gt;Using the eXpress Data Path (XDP) in Red Hat Enterprise Linux 8&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;XDP is based on &lt;a target="_blank" rel="nofollow" href="https://opensource.com/article/17/9/intro-ebpf"&gt;&lt;span class="aCOpRe"&gt;extended Berkeley Packet Filter (&lt;/span&gt;eBPF)&lt;/a&gt; and is still fast-moving. The eBPF/XDP coding format and style are also changing. So developers are creating tools and frameworks to make eBPF and XDP applications easy to write. Two of these resources, the &lt;a target="_blank" rel="nofollow" href="https://github.com/libbpf/libbpf.git"&gt;libbpf&lt;/a&gt; library and the &lt;a target="_blank" rel="nofollow" href="https://github.com/xdp-project/xdp-tools"&gt;xdp-tools &lt;/a&gt;utilities, are the topics of this article.&lt;/p&gt; &lt;p&gt;The article shows how to start writing XDP programs through the following tasks:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Write and run a small introductory program: &lt;ol&gt; &lt;li&gt;Write a program to drop all packets.&lt;/li&gt; &lt;li&gt;Build and view a BPF object.&lt;/li&gt; &lt;li&gt;Load a BPF object.&lt;/li&gt; &lt;li&gt;Show information on a running BPF object.&lt;/li&gt; &lt;li&gt;Unload a BPF object.&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Extend the program to let you deal with specific types of packets.&lt;/li&gt; &lt;li&gt;Use a packet counter to use BPF maps.&lt;/li&gt; &lt;li&gt;Add a customized userspace tool to load the BPF program.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The reader needs to be familiar with C code and IP header structures. All examples are tested with Red Hat Enterprise Linux (RHEL) 8.3.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To prepare the developing environment, install the following packages:&lt;/p&gt; &lt;pre&gt;$ sudo dnf install clang llvm gcc libbpf libbpf-devel libxdp libxdp-devel xdp-tools bpftool kernel-headers&lt;/pre&gt; &lt;h2&gt;Task 1: Write and run a simple program with XDP&lt;/h2&gt; &lt;p&gt;This section teaches the minimal tasks you need to use XDP.&lt;/p&gt; &lt;h3&gt;Task 1.1: Write a program to drop all packets&lt;/h3&gt; &lt;p&gt;Let&amp;#8217;s start with a simple XDP program in C:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;linux/bpf.h&amp;#62; #include &amp;#60;bpf/bpf_helpers.h&amp;#62; SEC("xdp_drop") int xdp_drop_prog(struct xdp_md *ctx) { return XDP_DROP; } char _license[] SEC("license") = "GPL";&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;linux/bpf.h&lt;/code&gt; header file is provided by the kernel-header package, which defines all the supported BPF helpers and xdp_actions, like the &lt;code&gt;XDP_DROP&lt;/code&gt; action used in this example.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;bpf/bpf_helpers.h&lt;/code&gt; header is provided by libbpf-devel, which provides some useful eBPF macros, like the &lt;code&gt;SEC&lt;/code&gt; macro used in this example. &lt;code&gt;SEC&lt;/code&gt;, short for &lt;em&gt;section,&lt;/em&gt; is used to place a fragment of the compiled object in different ELF sections, which we will see in later output from the &lt;code&gt;llvm-objdump&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;xdp_drop_prog()&lt;/code&gt; function takes a parameter &lt;code&gt;struct xdp_md *ctx&lt;/code&gt;, which we have not used yet. I will talk about it later. This function returns &lt;code&gt;XDP_DROP&lt;/code&gt;, which means we will drop all incoming packets. Other XDP actions include &lt;code&gt;XDP_PASS&lt;/code&gt;, &lt;code&gt;XDP_TX&lt;/code&gt;, and &lt;code&gt;XDP_REDIRECT&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Finally, the last line formally specifies the license associated with this program. Some eBPF helpers are accessible only by GPL-licensed programs, and the verifier will use this information to enforce such a restriction.&lt;/p&gt; &lt;h3&gt;Task 1.2: Build and dump the BPF object&lt;/h3&gt; &lt;p&gt;Let&amp;#8217;s build the program in the previous section with &lt;code&gt;clang&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ clang -O2 -g -Wall -target bpf -c xdp_drop.c -o xdp_drop.o &lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-O&lt;/code&gt; option specified which optimization level to use,  and&lt;code&gt;-g&lt;/code&gt; generates debugging information.&lt;/p&gt; &lt;p&gt;You can use &lt;code&gt;llvm-objdump&lt;/code&gt; to show the ELF format after the build. &lt;code&gt;llvm-objdump&lt;/code&gt; is very useful if you want to know what a program does and you don&amp;#8217;t have the source code. The &lt;code&gt;-h&lt;/code&gt; option displays the sections in the object, and the &lt;code&gt;-S&lt;/code&gt; option displays the source interleaved with the disassembled object code. We&amp;#8217;ll show each of those options in turn.&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ llvm-objdump -h xdp_drop.o &lt;/strong&gt; xdp_drop: file format ELF64-BPF Sections: Idx Name Size VMA Type 0 00000000 0000000000000000 1 .strtab 000000ad 0000000000000000 2 .text 00000000 0000000000000000 TEXT 3 xdp_drop 00000010 0000000000000000 TEXT 4 license 00000004 0000000000000000 DATA 5 .debug_str 00000125 0000000000000000 6 .debug_abbrev 000000ba 0000000000000000 7 .debug_info 00000114 0000000000000000 8 .rel.debug_info 000001c0 0000000000000000 9 .BTF 000001df 0000000000000000 10 .rel.BTF 00000010 0000000000000000 11 .BTF.ext 00000050 0000000000000000 12 .rel.BTF.ext 00000020 0000000000000000 13 .eh_frame 00000030 0000000000000000 DATA 14 .rel.eh_frame 00000010 0000000000000000 15 .debug_line 00000084 0000000000000000 16 .rel.debug_line 00000010 0000000000000000 17 .llvm_addrsig 00000002 0000000000000000 18 .symtab 000002d0 0000000000000000 &lt;strong&gt;$ llvm-objdump -S -no-show-raw-insn xdp_drop.o &lt;/strong&gt; xdp_drop: file format ELF64-BPF Disassembly of section xdp_drop: 0000000000000000 xdp_drop_prog: ; return XDP_DROP; 0: r0 = 1 1: exit &lt;/pre&gt; &lt;h3&gt;Task 1.3: Load a BPF object&lt;/h3&gt; &lt;p&gt;After you build the object, there are multiple ways to load it.&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Do not load test XDP programs on the default interface. Instead, use the &lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/#veth"&gt;veth interface&lt;/a&gt; for testing. This is advisable to protect you from losing network connectivity because the program is dropping packets.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;The easiest way to load the program is using the &lt;code&gt;ip&lt;/code&gt; command, like this:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo ip link set veth1 xdpgeneric obj xdp_drop.o sec xdp_drop &lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;But&lt;code&gt;ip&lt;/code&gt; doesn&amp;#8217;t support the &lt;a target="_blank" rel="nofollow" href="https://www.kernel.org/doc/html/latest/bpf/btf.html"&gt;BPF Type Format (BTF) &lt;/a&gt;type map that we will talk about later. Although the latest &lt;a target="_blank" rel="nofollow" href="https://git.kernel.org/pub/scm/network/iproute2/iproute2-next.git/commit/?id=f98ce50046b433687c0a661b6b9107a0603d1058"&gt;ip-next &lt;/a&gt;release has fixed this issue by adding libbpf support, it has not been backported to the main line yet.&lt;/p&gt; &lt;p&gt;The recommended way to load the XDP object on RHEL is using &lt;code&gt;xdp-loader&lt;/code&gt;. This command depends on libbpf, which has full BTF support, and is the only way to load multiple programs on one interface.&lt;/p&gt; &lt;p&gt;To get Red Hat support for XDP, use &lt;code&gt;libxdp&lt;/code&gt;, as explained in the article &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/8.3_release_notes/rhel-8-3-0-release#enhancement_networking"&gt;XDP is conditionally supported in RHEL 8.3 release notes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now let&amp;#8217;s load the object on interface &lt;code&gt;veth1&lt;/code&gt; with &lt;code&gt;xdp-loader&lt;/code&gt;. We specify &lt;code&gt;-m sbk&lt;/code&gt; to use skb mode. Other possible modes include native and offload. But because these modes are not supported on all NIC drivers, we will just use skb mode in this article. The&lt;code&gt;-s xdp_drop&lt;/code&gt; option specifies the use of the section we created, &lt;code&gt;xdp_drop&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo xdp-loader load -m skb -s xdp_drop veth1 xdp_drop.o &lt;/strong&gt;&lt;/pre&gt; &lt;h3&gt;Task 1.4: Show information on a running BPF object&lt;/h3&gt; &lt;p&gt;There are also multiple ways to show information about a loaded XDP program:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo xdp-loader status &lt;/strong&gt;CURRENT XDP PROGRAM STATUS: Interface Prio Program name Mode ID Tag Chain actions ------------------------------------------------------------------------------------- lo &amp;#60;no XDP program&amp;#62; ens3 &amp;#60;no XDP program&amp;#62; veth1 xdp_dispatcher skb 15 d51e469e988d81da =&amp;#62; 50 xdp_drop_prog 20 57cd311f2e27366b XDP_PASS $ &lt;strong&gt;sudo bpftool prog show&lt;/strong&gt; 15: xdp name xdp_dispatcher tag d51e469e988d81da gpl loaded_at 2021-01-13T03:24:43-0500 uid 0 xlated 616B jited 638B memlock 4096B map_ids 8 btf_id 12 20: ext name xdp_drop_prog tag 57cd311f2e27366b gpl loaded_at 2021-01-13T03:24:43-0500 uid 0 xlated 16B jited 40B memlock 4096B btf_id 16 $ &lt;strong&gt;sudo ip link show veth1&lt;/strong&gt; 4: veth1@if3: &amp;#60;BROADCAST,MULTICAST,UP,LOWER_UP&amp;#62; mtu 1500 xdpgeneric qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether ba:4d:98:21:3b:b3 brd ff:ff:ff:ff:ff:ff link-netns ns prog/xdp id 15 tag d51e469e988d81da jited &lt;/pre&gt; &lt;p&gt;If you load your program with &lt;code&gt;ip&lt;/code&gt; cmd, only one XDP program can be loaded simultaneously. If you load your program with&lt;code&gt;xdp-loader&lt;/code&gt;, two programs will be loaded by default. One is &lt;code&gt;xdp_dispatcher&lt;/code&gt;, created by &lt;code&gt;xdp_loader&lt;/code&gt;, and the other is &lt;code&gt;xdp_drop_prog&lt;/code&gt;, written by us. The second command issued above shows that &lt;code&gt;xdp_dispatcher&lt;/code&gt; is running with ID 15 and &lt;code&gt;xdp_drop_prog&lt;/code&gt; is running with ID 20.&lt;/p&gt; &lt;h3&gt;Task 1.5: Unload the XDP program&lt;/h3&gt; &lt;p&gt;If you use &lt;code&gt;ip cmd&lt;/code&gt; to load the program, you can unload the program through:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo ip link set veth1 xdpgeneric off &lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;Use the &lt;code&gt;xdp&lt;/code&gt; flag that corresponds to the way you loaded the file. In this example, we specified &lt;code&gt;xdpgeneric off&lt;/code&gt; because we loaded our program beginning with &lt;code&gt;ip link set veth1 xdpgeneric obj&lt;/code&gt;. Specify &lt;code&gt;xdp off&lt;/code&gt; if you loaded your beginning with &lt;code&gt;ip link set veth1 xdp obj&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To unload all XDP programs on an interface, issue &lt;code&gt;xdp-loader&lt;/code&gt; with the &lt;code&gt;-a&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo xdp-loader unload -a veth1 &lt;/strong&gt;&lt;/pre&gt; &lt;h2&gt;Task 2: Drop specific packets with XDP&lt;/h2&gt; &lt;p&gt;The first example dropped every packet, which has no practical use. Now let&amp;#8217;s do some real stuff. The example in this section drops all IPv6 packets:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;linux/bpf.h&amp;#62; #include &amp;#60;bpf/bpf_helpers.h&amp;#62; #include &amp;#60;linux/if_ether.h&amp;#62; #include &amp;#60;arpa/inet.h&amp;#62; SEC("xdp_drop") int xdp_drop_prog(struct xdp_md *ctx) { void *data_end = (void *)(long)ctx-&amp;#62;data_end; void *data = (void *)(long)ctx-&amp;#62;data; struct ethhdr *eth = data; __u16 h_proto; if (data + sizeof(struct ethhdr) &amp;#62; data_end) return XDP_DROP; h_proto = eth-&amp;#62;h_proto; if (h_proto == htons(ETH_P_IPV6)) return XDP_DROP; return XDP_PASS; } char _license[] SEC("license") = "GPL"; &lt;/pre&gt; &lt;p&gt;Compare the code just shown with the first program. Here we added two more header files:&lt;code&gt;linux/if_ether.h&lt;/code&gt; to get the&lt;code&gt;ethhdr&lt;/code&gt; struct and &lt;code&gt;arpa/inet.h&lt;/code&gt; to get the &lt;code&gt;htons()&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;struct xdp_md&lt;/code&gt; is also used in this example. It&amp;#8217;s defined (in Linux 5.10) like this:&lt;/p&gt; &lt;pre&gt;struct xdp_md { __u32 data; __u32 data_end; __u32 data_meta; /* Below access go through struct xdp_rxq_info */ __u32 ingress_ifindex; /* rxq-&amp;#62;dev-&amp;#62;ifindex */ __u32 rx_queue_index; /* rxq-&amp;#62;queue_index */ __u32 egress_ifindex; /* txq-&amp;#62;dev-&amp;#62;ifindex */ }; &lt;/pre&gt; &lt;p&gt;The packet contents lie between &lt;code&gt;ctx-&amp;#62;data&lt;/code&gt; and &lt;code&gt;ctx-&amp;#62;data_end&lt;/code&gt;. The data starts with an Ethernet header, so we assign the data to&lt;code&gt;ethhdr&lt;/code&gt; like this:&lt;/p&gt; &lt;pre&gt; void *data = (void *)(long)ctx-&amp;#62;data; struct ethhdr *eth = data; &lt;/pre&gt; &lt;p&gt;When accessing the data in &lt;code&gt;struct ethhdr&lt;/code&gt;, we must make sure we don&amp;#8217;t access invalid areas by checking whether &lt;code&gt;data + sizeof(struct ethhdr) &amp;#62; data_end&lt;/code&gt;, and returning without further action if it&amp;#8217;s true. This check is compulsory by the &lt;a target="_blank" rel="nofollow" href="https://www.kernel.org/doc/html/latest/networking/filter.html#ebpf-verifier"&gt;BPF verifer&lt;/a&gt; that verifies your program at runtime.&lt;/p&gt; &lt;p&gt;Then, determine whether the protocol in the Ethernet header is IPv6 by checking &lt;code&gt;h_proto == htons(ETH_P_IPV6)&lt;/code&gt;, and if it is, drop the packet by returning &lt;code&gt;XDP_DROP&lt;/code&gt;. For other packets, we just return &lt;code&gt;XDP_PASS&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Task 3: Map and count the processed packets&lt;/h2&gt; &lt;p&gt;In the previous example, we dropped IPv6 packets. In this example, we will keep track of how many packets we dropped. The example introduces another BPF feature: maps. BPF maps are used to share data between the kernel and userspace. We can update the map data in the kernel and read it from userspace, or vice versa.&lt;/p&gt; &lt;p&gt;Here is an example of a new BTF-defined map (introduced by upstream &lt;a target="_blank" rel="nofollow" href="https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?id=abd29c931459"&gt;commit abd29c931459&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;struct { __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY); __type(key, __u32); __type(value, long); __uint(max_entries, 1); } rxcnt SEC(".maps"); &lt;/pre&gt; &lt;p&gt;The map is named as &lt;code&gt;rxcnt&lt;/code&gt; with type &lt;code&gt;BPF_MAP_TYPE_PERCPU_ARRAY&lt;/code&gt;. This type indicates that we will have one instance of the map per CPU core; thus, if you have 4 cores, you will have 4 instances of the map. We will use each map to count how many packets are processed per core. The rest of the structure defines the key/value type and limits the maximum number entries to 1, because we need to count only one number (the number of received IPv6 packets). In C code, it would look like we defined an array on each CPU with a size of one, .e.g, &lt;code&gt;unsigned int rxcnt[1]&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Our program looks up the value of the &lt;code&gt;rxcnt&lt;/code&gt; entry with the function &lt;code&gt;bpf_map_lookup_elem()&lt;/code&gt; and updates the value. Here is the full code:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;linux/bpf.h&amp;#62; #include &amp;#60;bpf/bpf_helpers.h&amp;#62; #include &amp;#60;linux/if_ether.h&amp;#62; #include &amp;#60;arpa/inet.h&amp;#62; struct { __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY); __type(key, __u32); __type(value, long); __uint(max_entries, 1); } rxcnt SEC(".maps"); SEC("xdp_drop_ipv6") int xdp_drop_ipv6_prog(struct xdp_md *ctx) { void *data_end = (void *)(long)ctx-&amp;#62;data_end; void *data = (void *)(long)ctx-&amp;#62;data; struct ethhdr *eth = data; __u16 h_proto; __u32 key = 0; long *value; if (data + sizeof(struct ethhdr) &amp;#62; data_end) return XDP_DROP; h_proto = eth-&amp;#62;h_proto; if (h_proto == htons(ETH_P_IPV6)) { value = bpf_map_lookup_elem(&amp;#38;rxcnt, &amp;#38;key); if (value) *value += 1; return XDP_DROP; } return XDP_PASS; } char _license[] SEC("license") = "GPL"; &lt;/pre&gt; &lt;p&gt;Let&amp;#8217;s name the new program &lt;code&gt;xdp_drop_ipv6_count.c&lt;/code&gt; and build it to create the object file &lt;code&gt;xdp_drop_ipv6_count.o&lt;/code&gt;. After loading the object, send some IPv6 packets to this interface. Using the &lt;code&gt;bpftool map show&lt;/code&gt; command, we can see that the &lt;code&gt;rxcnt&lt;/code&gt; ID in our map is 13. Then we can use &lt;code&gt;bpftool map dump id 13&lt;/code&gt; to show that 13 packets were processed on CPU 0 and 7 packets were processed on CPU 1:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo xdp-loader load -m skb -s xdp_drop_ipv6 veth1 xdp_drop_ipv6_count.o &lt;/strong&gt; ...&lt;em&gt;receive some IPv6 packets&lt;/em&gt; &lt;strong&gt;$ sudo bpftool map show &lt;/strong&gt;bpftool map show 13: percpu_array name rxcnt flags 0x0 key 4B value 8B max_entries 1 memlock 4096B btf_id 20 19: array name xdp_disp.rodata flags 0x480 key 4B value 84B max_entries 1 memlock 8192B btf_id 28 frozen # bpftool map dump id 13 [{ "key": 0, "values": [{ "cpu": 0, "value": 13 },{ "cpu": 1, "value": 7 },{ "cpu": 2, "value": 0 },{ "cpu": 3, "value": 0 } ] } ] &lt;/pre&gt; &lt;p&gt;BPF supports many more &lt;a target="_blank" rel="nofollow" href="https://prototype-kernel.readthedocs.io/en/latest/bpf/ebpf_maps_types.html"&gt;map types&lt;/a&gt;, such as BPF_MAP_TYPE_HASH, BPF_MAP_TYPE_ARRAY, etc.&lt;/p&gt; &lt;h2&gt;Task 4: Load XDP objects with the custom loader&lt;/h2&gt; &lt;p&gt;We can load the XDP objects with&lt;code&gt;ip&lt;/code&gt; and show the map number with &lt;code&gt;bpftool&lt;/code&gt;. But if we want more advanced features (to create, read, and write maps, attach XDP programs to interfaces, etc.), we need to write the loader ourselves.&lt;/p&gt; &lt;p&gt;Here is an example of how to show the total packets and the number of packets per second (PPS) we dropped. In this example, I hard-coded a lot of stuff such as kernel object name, section name, etc. These all can be set via parameters in your own code.&lt;/p&gt; &lt;p&gt;The purpose of each function is explained by comments in the code:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;unistd.h&amp;#62; #include &amp;#60;stdlib.h&amp;#62; #include &amp;#60;string.h&amp;#62; #include &amp;#60;linux/if_link.h&amp;#62; #include &amp;#60;signal.h&amp;#62; #include &amp;#60;net/if.h&amp;#62; #include &amp;#60;assert.h&amp;#62; /* In this example we use libbpf-devel and libxdp-devel */ #include &amp;#60;bpf/bpf.h&amp;#62; #include &amp;#60;bpf/libbpf.h&amp;#62; #include &amp;#60;xdp/libxdp.h&amp;#62; /* We define the following global variables */ static int ifindex; struct xdp_program *prog = NULL; /* This function will remove XDP from the link when the program exits. */ static void int_exit(int sig) { xdp_program__close(prog); exit(0); } /* This function will count the per-CPU number of packets and print out * the total number of dropped packets number and PPS (packets per second). */ static void poll_stats(int map_fd, int interval) { int ncpus = libbpf_num_possible_cpus(); if (ncpus &amp;#60; 0) { printf("Error get possible cpus\n"); return; } long values[ncpus], prev[ncpus], total_pkts; int i, key = 0; memset(prev, 0, sizeof(prev)); while (1) { long sum = 0; sleep(interval); assert(bpf_map_lookup_elem(map_fd, &amp;#38;key, values) == 0); for (i = 0; i &amp;#60; ncpus; i++) sum += (values[i] - prev[i]); if (sum) { total_pkts += sum; printf("total dropped %10llu, %10llu pkt/s\n", total_pkts, sum / interval); } memcpy(prev, values, sizeof(values)); } } int main(int argc, char *argv[]) { int prog_fd, map_fd, ret; struct bpf_object *bpf_obj; if (argc != 2) { printf("Usage: %s IFNAME\n", argv[0]); return 1; } ifindex = if_nametoindex(argv[1]); if (!ifindex) { printf("get ifindex from interface name failed\n"); return 1; } /* load XDP object by libxdp */ prog = xdp_program__open_file("xdp_drop_ipv6_count.o", "xdp_drop_ipv6", NULL); if (!prog) { printf("Error, load xdp prog failed\n"); return 1; } /* attach XDP program to interface with skb mode * Please set ulimit if you got an -EPERM error. */ ret = xdp_program__attach(prog, ifindex, XDP_MODE_SKB, 0); if (ret) { printf("Error, Set xdp fd on %d failed\n", ifindex); return ret; } /* Find the map fd from the bpf object */ bpf_obj = xdp_program__bpf_obj(prog); map_fd = bpf_object__find_map_fd_by_name(bpf_obj, "rxcnt"); if (map_fd &amp;#60; 0) { printf("Error, get map fd from bpf obj failed\n"); return map_fd; } /* Remove attached program when it is interrupted or killed */ signal(SIGINT, int_exit); signal(SIGTERM, int_exit); poll_stats(map_fd, 2); return 0; } &lt;/pre&gt; &lt;p&gt;Set the &lt;code&gt;ulimit&lt;/code&gt; to unlimited and build the program with the &lt;code&gt;-lbpf -lxdp&lt;/code&gt; flags. Then run the program, which shows the packet count output:&lt;/p&gt; &lt;pre&gt;&lt;strong&gt;$ sudo ulimit -l unlimited $ gcc xdp_drop_ipv6_count_user.c -o xdp_drop_ipv6_count -lbpf -lxdp $ sudo ./xdp_drop_ipv6_count veth1&lt;/strong&gt; total dropped 2, 1 pkt/s total dropped 129, 63 pkt/s total dropped 311, 91 pkt/s total dropped 492, 90 pkt/s total dropped 674, 91 pkt/s total dropped 856, 91 pkt/s total dropped 1038, 91 pkt/s ^C &lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article helps you understand what a XDP program looks like, how to add a BPF map, and how to write a custom loader. To learn more about XDP programming, please visit &lt;a target="_blank" rel="nofollow" href="https://github.com/xdp-project/xdp-tutorial"&gt;xdp-tutorial&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#38;linkname=Get%20started%20with%20XDP" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F01%2Fget-started-with-xdp%2F&amp;#038;title=Get%20started%20with%20XDP" data-a2a-url="https://developers.redhat.com/blog/2021/04/01/get-started-with-xdp/" data-a2a-title="Get started with XDP"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/01/get-started-with-xdp/"&gt;Get started with XDP&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/eiVNaOvOmFg" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;XDP (eXpress Data Path) is a powerful new networking feature in Linux that enables high-performance programmable access to networking packets before they enter the networking stack. But XDP has a high learning curve. Many developers have written introduction blogs for this feature, such as Paolo Abeni&amp;#8217;s Achieving high-performance, low-latency networking with XDP: Part I and [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/01/get-started-with-xdp/"&gt;Get started with XDP&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/01/get-started-with-xdp/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">857207</post-id><dc:creator>Hangbin Liu</dc:creator><dc:date>2021-04-01T07:00:13Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/01/get-started-with-xdp/</feedburner:origLink></entry></feed>
